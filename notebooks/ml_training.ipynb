{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41976788",
   "metadata": {},
   "source": [
    "# Análisis de historial de predicciones tácticas (`predicciones.csv`)\n",
    "\n",
    "Este notebook explora el historial generado por el modelo de predicción de errores tácticos en `chess_trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/predicciones.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb08b2",
   "metadata": {},
   "source": [
    "## Frecuencia de etiquetas tácticas predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"predicted_label\", order=df[\"predicted_label\"].value_counts().index)\n",
    "plt.title(\"Distribución de etiquetas tácticas\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbd088",
   "metadata": {},
   "source": [
    "## Evolución temporal por etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_por_fecha = df.groupby(df[\"timestamp\"].dt.date)[\"predicted_label\"].value_counts().unstack().fillna(0)\n",
    "etiquetas_por_fecha.plot(kind=\"bar\", stacked=True, figsize=(12, 6))\n",
    "plt.title(\"Etiquetas tácticas por día\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301bb4d",
   "metadata": {},
   "source": [
    "## Boxplot de score_diff por etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x=\"predicted_label\", y=\"score_diff\")\n",
    "plt.title(\"score_diff por tipo de error predicho\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4efd73",
   "metadata": {},
   "source": [
    "## Histograma de branching_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"branching_factor\", bins=20, kde=True)\n",
    "plt.title(\"Distribución del branching_factor\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a89baa",
   "metadata": {},
   "source": [
    "## Correlaciones numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b60d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Mapa de calor de correlaciones\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903e803",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de6c6a",
   "metadata": {},
   "source": [
    "- Se observan más errores graves (score_diff negativos) en jugadas con menor branching_factor.\n",
    "- Las etiquetas tácticas más frecuentes son las intermedias como 'Error' o 'Aceptable'.\n",
    "- Hay estabilidad en la distribución temporal, aunque algunos días muestran concentración de errores impulsivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4612da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "kaggle datasets download ronakbadhe/chess-evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fecfa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00f0a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers\n",
    "import pandas as pd\n",
    "import chess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c23a08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "piece_to_index = {\n",
    "    'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "    'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "}\n",
    "\n",
    "# Helper functions to convert FEN to tensor\n",
    "def fen_to_tensor(fen):\n",
    "    board = chess.Board(fen)\n",
    "    board_tensor = np.zeros((13, 8, 8), dtype=np.float32)\n",
    "    \n",
    "    # Castling mapping\n",
    "    castling_map = {'K': (7, 6), 'Q': (7, 2), 'k': (0, 6), 'q': (0, 2)}\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            row, col = 7 - chess.square_rank(square), chess.square_file(square)\n",
    "            board_tensor[piece_to_index[piece.symbol()], row, col] = 1\n",
    "\n",
    "    # FEN features\n",
    "    fen_parts = fen.split()\n",
    "    active_player = 1 if fen_parts[1] == 'w' else 0\n",
    "    halfmove_clock = float(fen_parts[4]) / 100.0\n",
    "    en_passant = fen_parts[3]\n",
    "    castle_rights = fen_parts[2]\n",
    "    \n",
    "    # Encode en passant\n",
    "    if en_passant != '-':\n",
    "        row, col = 7 - (int(en_passant[1]) - 1), ord(en_passant[0]) - ord('a')\n",
    "        board_tensor[12, row, col] = 1\n",
    "\n",
    "    # Encode castling rights\n",
    "    if castle_rights != '-':\n",
    "        for right in castle_rights:\n",
    "            row, col = castling_map[right]\n",
    "            board_tensor[12, row, col] = 1\n",
    "\n",
    "    return board_tensor, active_player, halfmove_clock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a15135",
   "metadata": {},
   "source": [
    "## Load dataset from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fad370",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data(csv_path,sample_size=1000):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data = data.sample(n=sample_size, random_state=42)\n",
    "    boards, active_players, halfmove_clocks, evaluations = [], [], [], []\n",
    "    for idx, row in data.iterrows():\n",
    "        board_tensor, active_player, halfmove_clock = fen_to_tensor(row['FEN'])\n",
    "        boards.append(board_tensor)\n",
    "        active_players.append(active_player)\n",
    "        halfmove_clocks.append(halfmove_clock)\n",
    "        evaluation=row['Evaluation']\n",
    "\n",
    "        if evaluation.startswith('#'):\n",
    "            # Converting checkmate to large positive/negative values\n",
    "            if evaluation[1] == '-':\n",
    "                # Negative checkmate (opponent checkmating)\n",
    "                evaluation = -10000.0  # Arbitrary large negative value\n",
    "            else:\n",
    "                # Positive checkmate (current player checkmating)\n",
    "                evaluation = 10000.0  # Arbitrary large positive value\n",
    "        else:\n",
    "            # Standard centipawn evaluation to float\n",
    "            evaluation = float(evaluation)\n",
    "        \n",
    "        evaluations.append(evaluation)\n",
    "\n",
    "    boards = np.array(boards)\n",
    "    active_players = np.array(active_players)\n",
    "    halfmove_clocks = np.array(halfmove_clocks)\n",
    "    evaluations = np.array(evaluations)\n",
    "    return boards, active_players, halfmove_clocks, evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a027b7",
   "metadata": {},
   "source": [
    "## Data loader + Label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf55fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Conditional Batch Norm Layer\n",
    "class ConditionalBatchNorm(layers.Layer):\n",
    "    def __init__(self, num_features, num_conditions):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.bn = layers.BatchNormalization(center=False, scale=False)\n",
    "        self.gamma = layers.Embedding(num_conditions, num_features, embeddings_initializer='ones')\n",
    "        self.beta = layers.Embedding(num_conditions, num_features, embeddings_initializer='zeros')\n",
    "\n",
    "    def call(self, x, condition):\n",
    "        normalized = self.bn(x)\n",
    "        gamma = self.gamma(condition)[:, tf.newaxis, tf.newaxis, :]\n",
    "        beta = self.beta(condition)[:, tf.newaxis, tf.newaxis, :]\n",
    "        return gamma * normalized + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ace0f9",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "**Conditional Batch Normalization**\n",
    "Used to distinguish between black & white turns to play when training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f986cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class ChessEvaluationCNN(Model):\n",
    "    def __init__(self, num_piece_channels=13, num_classes=1, num_conditions=2):\n",
    "        super(ChessEvaluationCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=3, padding='same')\n",
    "        self.cbn1 = ConditionalBatchNorm(64, num_conditions)\n",
    "        self.conv2 = layers.Conv2D(128, kernel_size=3, padding='same')\n",
    "        self.cbn2 = ConditionalBatchNorm(128, num_conditions)\n",
    "        self.conv3 = layers.Conv2D(256, kernel_size=3, padding='same')\n",
    "        self.cbn3 = ConditionalBatchNorm(256, num_conditions)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(1024, activation='relu')\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        board_tensor, active_player, halfmove_clock = inputs\n",
    "\n",
    "        # Forward pass\n",
    "        x = self.conv1(board_tensor)\n",
    "        x = self.cbn1(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.cbn2(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.cbn3(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = tf.reduce_mean(x, axis=[1, 2])  # (batch_size, 256)\n",
    "        \n",
    "        # Fully connected layer with halfmove clock\n",
    "        x = tf.concat([self.fc1(x), tf.expand_dims(halfmove_clock, -1)], axis=1)\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a522c64",
   "metadata": {},
   "source": [
    "## Original pure CNN architecture:\n",
    "This version used a convultional network with a kernel size of 3 to learn the position's features. IN theory, useful for local features identification like pawn chains and structures, but can't make sense of long range relationships like threats, pins and attacks .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2850c9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class ChessEvaluationCNN(Model):\n",
    "    def __init__(self, num_piece_channels=13, num_classes=1, num_conditions=2):\n",
    "        super(ChessEvaluationCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=3, padding='same')\n",
    "        self.cbn1 = ConditionalBatchNorm(64, num_conditions)\n",
    "        self.conv2 = layers.Conv2D(128, kernel_size=3, padding='same')\n",
    "        self.cbn2 = ConditionalBatchNorm(128, num_conditions)\n",
    "        self.conv3 = layers.Conv2D(256, kernel_size=3, padding='same')\n",
    "        self.cbn3 = ConditionalBatchNorm(256, num_conditions)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(1024, activation='relu')\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        board_tensor, active_player, halfmove_clock = inputs\n",
    "\n",
    "        # Forward pass\n",
    "        x = self.conv1(board_tensor)\n",
    "        x = self.cbn1(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.cbn2(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.cbn3(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = tf.reduce_mean(x, axis=[1, 2])  # (batch_size, 256)\n",
    "        \n",
    "        # Fully connected layer with halfmove clock\n",
    "        x = tf.concat([self.fc1(x), tf.expand_dims(halfmove_clock, -1)], axis=1)\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac7e5e",
   "metadata": {},
   "source": [
    "## CNN + VIT\n",
    "Using a hybrid architecture consisting of convolutional network + vision transformer with the added benefit of self attention, giving the model the ability to learn long range piece relationships, highly scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5be34f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to create patches for ViT\n",
    "def create_patches(x, patch_size):\n",
    "    # Dynamically get batch size and input dimensions\n",
    "    batch_size = tf.shape(x)[0]  # Dynamically fetch the actual batch size at runtime\n",
    "    channels = x.shape[1]  # Channels are known statically (13)\n",
    "    height = x.shape[2]     # Known statically (8)\n",
    "    width = x.shape[3]      # Known statically (8)\n",
    "\n",
    "    # Ensure the input is in the expected shape\n",
    "    if height != 8 or width != 8:\n",
    "        raise ValueError(\"Input dimensions for chessboard must be (None, 13, 8, 8)\")\n",
    "\n",
    "    # Reshape the input tensor to create patches\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=tf.transpose(x, [0, 2, 3, 1]),  \n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "\n",
    "    # Reshape the patches into (batch_size, num_patches, patch_dim)\n",
    "    patch_dim = patch_size * patch_size * channels \n",
    "    num_patches = (height // patch_size) * (width // patch_size)\n",
    "    \n",
    "    # Use static shape where possible to avoid runtime errors during XLA compilation\n",
    "    patches = tf.reshape(patches, [-1, num_patches, patch_dim])  # Use -1 for dynamic batch_size\n",
    "\n",
    "    return patches\n",
    "\n",
    "class ViTBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, embed_dim, ff_dim):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "# Hybrid model definition (CNN + ViT)\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable(package=\"ChessModel\")\n",
    "class ChessEvaluationHybridModel(Model):\n",
    "    def __init__(self, num_piece_channels=13, num_classes=1, num_conditions=2, patch_size=2):\n",
    "        super(ChessEvaluationHybridModel, self).__init__()\n",
    "        \n",
    "        self.num_piece_channels=num_piece_channels\n",
    "        self.num_classes=num_classes\n",
    "        self.num_conditions=num_conditions\n",
    "        self.patch_size=patch_size\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=3, padding='same')\n",
    "        self.cbn1 = ConditionalBatchNorm(64, num_conditions)\n",
    "        self.conv2 = layers.Conv2D(128, kernel_size=3, padding='same')\n",
    "        self.cbn2 = ConditionalBatchNorm(128, num_conditions)\n",
    "        self.conv3 = layers.Conv2D(256, kernel_size=3, padding='same')\n",
    "        self.cbn3 = ConditionalBatchNorm(256, num_conditions)\n",
    "        \n",
    "        # ViT layers\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = (patch_size * patch_size) * num_piece_channels\n",
    "        self.vit_proj = layers.Dense(self.embedding_dim)  # Project patches into embedding space\n",
    "        \n",
    "        self.vit_block1 = ViTBlock(num_heads=4, embed_dim=self.embedding_dim, ff_dim=512)\n",
    "        self.vit_block2 = ViTBlock(num_heads=4, embed_dim=self.embedding_dim, ff_dim=512)\n",
    "        \n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = layers.Dense(1024, activation='relu')\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        board_tensor, active_player, halfmove_clock = inputs\n",
    "\n",
    "        # CNN forward pass\n",
    "        x = self.conv1(board_tensor)\n",
    "        x = self.cbn1(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.cbn2(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.cbn3(x, active_player)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ViT forward pass\n",
    "        patches = create_patches(board_tensor, self.patch_size)\n",
    "        patches = self.vit_proj(patches)\n",
    "        vit_out = self.vit_block1(patches)\n",
    "        vit_out = self.vit_block2(vit_out)\n",
    "        vit_out = tf.reduce_mean(vit_out, axis=1)  # Global average pooling for patches\n",
    "\n",
    "        # Combine CNN and ViT outputs\n",
    "        x = tf.concat([self.flatten(x), vit_out], axis=1)\n",
    "        \n",
    "        # Fully connected layers with halfmove clock\n",
    "        x = tf.concat([self.fc1(x), tf.expand_dims(halfmove_clock, -1)], axis=1)\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Manually pass in the parameters here\n",
    "        return cls(\n",
    "            num_piece_channels=config['num_piece_channels'],\n",
    "            num_classes=config['num_classes'],\n",
    "            num_conditions=config['num_conditions'],\n",
    "            patch_size=config['patch_size']\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Include the custom arguments in the config dictionary\n",
    "        config.update({\n",
    "            'num_piece_channels': self.num_piece_channels,\n",
    "            'num_classes': self.num_classes,\n",
    "            'num_conditions': self.num_conditions,\n",
    "            'patch_size': self.patch_size\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10060752",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Loading previous version output using kaggle api \n",
    "!rm -rf /kaggle/working/*\n",
    "\n",
    "#Replace with your own kaggle api as kaggle secrets input when copying this noteebook\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "Api_key = user_secrets.get_secret(\"Kaggle Api\")\n",
    "\n",
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = 'oussamahaboubi'\n",
    "os.environ['KAGGLE_KEY'] = Api_key\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    saved_model=None\n",
    "    home_path=\"/kaggle/working/\"\n",
    "    for file in os.listdir(home_path):\n",
    "        if \".keras\" in file:\n",
    "            saved_model=home_path+file\n",
    "    return saved_model\n",
    "\n",
    "saved_model=get_model()\n",
    "if not saved_model:   \n",
    "    !kaggle kernels output oussamahaboubi/chess-evaluation-cnn-tensorflow -p /kaggle/working/\n",
    "    saved_model=get_model()\n",
    "\n",
    "# from tensorflow.keras.models import load_model\n",
    "# print(saved_model)\n",
    "\n",
    "# saved_model='/kaggle/working/checkpoint211120241650.model.keras'\n",
    "# model = ChessEvaluationHybridModel()\n",
    "# model = tf.keras.models.load_model(saved_model,custom_objects={'ChessEvaluationHybridModel': ChessEvaluationHybridModel})\n",
    "\n",
    "# Create model with manual config\n",
    "# model = ChessEvaluationHybridModel()\n",
    "# model=tf.keras.models.load_model(\"/kaggle/working/21-11-2024 19:51.keras\")\n",
    "\n",
    "# Now load the weights into the new model\n",
    "# model.load_weights(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6bb97",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a82f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Append the loss at the end of each epoch\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d386a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare TensorFlow dataset\n",
    "SAMPLE_SIZE=1000000\n",
    "EPOCHS=200\n",
    "BATCH_SIZE=512\n",
    "boards, active_players, halfmove_clocks, evaluations = load_data('/kaggle/input/chess-evaluations/random_evals.csv',sample_size=SAMPLE_SIZE)\n",
    "\n",
    "inputs = (boards, active_players, halfmove_clocks)\n",
    "targets = evaluations\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "dataset = dataset.shuffle(buffer_size=2048).batch(BATCH_SIZE)\n",
    "\n",
    "loss_history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c92c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint_filepath = \"model.keras\"#f'checkpoint{now}.model.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.005,\n",
    "    decay_steps=20000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer=optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "# Compile and train the model\n",
    "model = ChessEvaluationHybridModel()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "model.load_weights(saved_model)\n",
    "\n",
    "\n",
    "model.fit(dataset,epochs=EPOCHS, callbacks=[loss_history,model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2153172",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Epoch': range(1, EPOCHS + 1), 'Loss': loss_history.losses})\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(df['Epoch'], df['Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Logarithmic Scale)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5bb59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "model.save(f'/kaggle/working/{now}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4062d",
   "metadata": {},
   "source": [
    "## Testing Saved Models\n",
    "Latest Checkpoint\n",
    "using the latest checkpoint generated with model.Save, which ended at around 24k loss value and rising, reassigning the optimizer starts from a higher loss ,around 230K and decreasing quickly. But not as bad as training the model from the ground up.\n",
    "\n",
    "These losses sugget possible issues like local minima, overfitting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8131d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model2=tf.keras.models.load_model(\"/kaggle/working/22-11-2024 00:41:15.keras\", custom_objects={\"ChessEvaluationHybridModel\":ChessEvaluationHybridModel})\n",
    "\n",
    "lr_schedule2 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=20000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer2=optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model2.compile(optimizer=optimizer2, loss='mse')\n",
    "model2.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc665a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model3=tf.keras.models.load_model(\"/kaggle/working/22-11-2024 00:41:14.keras\", custom_objects={\"ChessEvaluationHybridModel\":ChessEvaluationHybridModel})\n",
    "\n",
    "model3.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13117b5b",
   "metadata": {},
   "source": [
    "## Best checkpoint\n",
    "Using the callback checkpoint allows us to continue training from the lowest loss point of the model, However this still runs into the issue of increasing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0003e94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model4=tf.keras.models.load_model(\"/kaggle/working/model.keras\", custom_objects={\"ChessEvaluationHybridModel\":ChessEvaluationHybridModel})\n",
    "\n",
    "model4.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16eeb95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model5=tf.keras.models.load_model(\"/kaggle/working/model.keras\", custom_objects={\"ChessEvaluationHybridModel\":ChessEvaluationHybridModel})\n",
    "\n",
    "lr_schedule3 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=20000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer3=optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model5.compile(optimizer=optimizer3, loss='mse')\n",
    "model5.fit(dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de543cec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396a8580",
   "metadata": {},
   "source": [
    "# Modelo Supervisado para Predecir error_label\n",
    "Este notebook entrena un modelo de clasificación para predecir el tipo de error cometido en una jugada de ajedrez usando los features generados en `training_dataset.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718802c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Inspección inicial\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df['error_label'].value_counts())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Selección de features\n",
    "features = [\n",
    "    'score_diff', 'material_total', 'material_balance', 'num_pieces',\n",
    "    'branching_factor', 'self_mobility', 'opponent_mobility',\n",
    "    'phase', 'has_castling_rights', 'is_low_mobility', \n",
    "    'is_center_controlled', 'is_pawn_endgame'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['error_label']\n",
    "\n",
    "# División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Entrenamiento\n",
    "clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "joblib.dump(clf, 'trained_error_label_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8390ca9",
   "metadata": {},
   "source": [
    "## Comparación con otros modelos (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(f\"{name} accuracy: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae02b75",
   "metadata": {},
   "source": [
    "# PCA + Clustering en Partidas de Ajedrez\n",
    "Este notebook aplica PCA a posiciones vectorizadas y agrupa los errores en clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fde1f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Ensure the 'src' directory exists and is correctly added to the Python path\u001b[39;00m\n\u001b[32m      6\u001b[39m src_path = os.path.abspath(\u001b[33m'\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "# Ensure the 'src' directory exists and is correctly added to the Python path\n",
    "src_path = os.path.abspath('src')\n",
    "if os.path.exists(src_path) and src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "from src.extractor import extract_features_from_fen\n",
    "from src.reducer import apply_pca\n",
    "from src.cluster import cluster_points\n",
    "from src.utils import load_pgn_positions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the 'src' directory is in the Python path\n",
    "sys.path.append(os.path.abspath('src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar posiciones\n",
    "fens = load_pgn_positions('data/game.png')\n",
    "X = np.array([extract_features_from_fen(f) for f in fens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA\n",
    "pca, Z = apply_pca(X, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "labels, model = cluster_points(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=labels, cmap='viridis', s=30)\n",
    "plt.title('Clusters de posiciones en PCA')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def9387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "PovScore(Cp(+118), WHITE) PovScore(Cp(+105), WHITE) PovScore(Cp(+91), WHITE)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chess\n",
    "import chess.engine\n",
    "import dotenv\n",
    "env = dotenv.load_dotenv()\n",
    "\n",
    "STOCKFISH_PATH = os.environ.get(\"STOCKFISH_PATH\",\"/usr/local/bin’\")\n",
    "\n",
    "with chess.engine.SimpleEngine.popen_uci(STOCKFISH_PATH) as engine:\n",
    "    board = chess.Board(\"rnbqkb1r/pppp2pp/5n2/4pP2/8/5N2/PPPP1PPP/RNBQKB1R w KQkq - 1 4\")\n",
    "    info = engine.analyse(board, chess.engine.Limit(depth=10), multipv=3)\n",
    "    print(type(info))  # → <class 'list'>\n",
    "    print(info[0][\"score\"], info[1][\"score\"], info[2][\"score\"])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
