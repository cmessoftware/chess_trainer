"""
ğŸš€ Script de AnÃ¡lisis de Datasets Disponibles
Analiza todos los datasets existentes para predicciones ML
"""

import pandas as pd
import os
from pathlib import Path
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_datasets():
    """Analizar todos los datasets disponibles"""
    
    print("ğŸ” ANÃLISIS DE DATASETS DISPONIBLES")
    print("=" * 50)
    
    # Rutas de datasets
    DATA_DIR = Path("data/export")
    
    datasets = {
        "unified_all": DATA_DIR / "unified_all_sources.parquet",
        "unified_small": DATA_DIR / "unified_small_sources.parquet",
        "elite": DATA_DIR / "elite",
        "fide": DATA_DIR / "fide", 
        "novice": DATA_DIR / "novice",
        "personal": DATA_DIR / "personal",
        "stockfish": DATA_DIR / "stockfish"
    }
    
    summary = {}
    
    for name, path in datasets.items():
        print(f"\nğŸ“Š Dataset: {name}")
        print("-" * 30)
        
        try:
            if path.is_dir():
                # Directorio con mÃºltiples archivos
                parquet_files = list(path.glob("*.parquet"))
                if parquet_files:
                    print(f"   ğŸ“ Directorio con {len(parquet_files)} archivos .parquet")
                    
                    # Analizar primer archivo como muestra
                    sample_file = parquet_files[0]
                    df_sample = pd.read_parquet(sample_file)
                    
                    print(f"   ğŸ“„ Archivo muestra: {sample_file.name}")
                    print(f"   ğŸ“ Shape muestra: {df_sample.shape}")
                    
                    if 'error_label' in df_sample.columns:
                        print(f"   ğŸ¯ Target distribution:")
                        target_dist = df_sample['error_label'].value_counts()
                        for label, count in target_dist.items():
                            print(f"      {label}: {count}")
                    
                    summary[name] = {
                        'type': 'directory',
                        'files': len(parquet_files),
                        'sample_shape': df_sample.shape,
                        'columns': list(df_sample.columns)
                    }
                else:
                    print(f"   âŒ No se encontraron archivos .parquet")
                    
            elif path.exists() and path.suffix == '.parquet':
                # Archivo Ãºnico
                df = pd.read_parquet(path)
                print(f"   ğŸ“Š Shape: {df.shape}")
                print(f"   ğŸ“ˆ Columnas: {len(df.columns)}")
                
                # AnÃ¡lisis de target
                if 'error_label' in df.columns:
                    print(f"   ğŸ¯ Target distribution:")
                    target_dist = df['error_label'].value_counts()
                    for label, count in target_dist.items():
                        print(f"      {label}: {count} ({count/len(df)*100:.1f}%)")
                
                # AnÃ¡lisis de missing values
                missing = df.isnull().sum().sum()
                print(f"   â“ Missing values: {missing} ({missing/(df.shape[0]*df.shape[1])*100:.1f}%)")
                
                # Features mÃ¡s importantes
                feature_cols = [col for col in df.columns 
                              if col not in ['error_label', 'pgn', 'game_id', 'move_san']]
                print(f"   ğŸ”§ Features disponibles: {len(feature_cols)}")
                
                # Features tÃ¡cticas
                tactical_features = [col for col in df.columns 
                                   if any(term in col.lower() for term in 
                                         ['depth_score', 'threatens_mate', 'forced_move', 'tactical'])]
                if tactical_features:
                    print(f"   âš”ï¸ Features tÃ¡cticas: {len(tactical_features)}")
                    print(f"      {tactical_features[:5]}{'...' if len(tactical_features) > 5 else ''}")
                
                # ELO standardization
                elo_cols = [col for col in df.columns if 'elo' in col.lower()]
                if elo_cols:
                    print(f"   ğŸ¯ Columnas ELO: {elo_cols}")
                
                summary[name] = {
                    'type': 'file',
                    'shape': df.shape,
                    'columns': list(df.columns),
                    'missing_values': missing,
                    'features': len(feature_cols),
                    'tactical_features': len(tactical_features),
                    'elo_columns': elo_cols
                }
                
            else:
                print(f"   âŒ No existe: {path}")
                
        except Exception as e:
            print(f"   âŒ Error al analizar: {e}")
    
    # Reporte final
    print(f"\nğŸ“‹ RESUMEN FINAL")
    print("=" * 50)
    
    ready_datasets = []
    for name, info in summary.items():
        if info:
            if info['type'] == 'file':
                ready_datasets.append(name)
                print(f"âœ… {name}: {info['shape'][0]:,} samples, {info['features']} features")
            elif info['type'] == 'directory':
                ready_datasets.append(name)
                print(f"âœ… {name}: {info['files']} archivos, {info['sample_shape'][0]:,} samples (muestra)")
    
    print(f"\nğŸ¯ Datasets listos para ML: {len(ready_datasets)}")
    print(f"ğŸ“Š RecomendaciÃ³n: Usar 'unified_all' para mÃ¡ximo rendimiento")
    
    return summary

def recommend_dataset_strategy(summary):
    """Recomendar estrategia basada en anÃ¡lisis"""
    
    print(f"\nğŸ¯ ESTRATEGIA RECOMENDADA")
    print("=" * 50)
    
    # Identificar mejor dataset
    if 'unified_all' in summary and summary['unified_all']:
        best_dataset = 'unified_all'
        info = summary['unified_all']
        print(f"ğŸ† Dataset principal: {best_dataset}")
        print(f"   ğŸ“Š {info['shape'][0]:,} samples para entrenamiento")
        print(f"   ğŸ”§ {info['features']} features disponibles")
        
        if info['tactical_features'] > 0:
            print(f"   âš”ï¸ {info['tactical_features']} features tÃ¡cticas - Â¡Excelente!")
        
        print(f"\nğŸ“‹ Plan de ExperimentaciÃ³n:")
        print(f"   1. ğŸ¯ Entrenamiento bÃ¡sico con {best_dataset}")
        print(f"   2. ğŸ“ˆ ComparaciÃ³n por fuentes (elite, novice, personal)")
        print(f"   3. âš™ï¸ OptimizaciÃ³n de hiperparÃ¡metros")
        
        if info['tactical_features'] > 0:
            print(f"   4. âš”ï¸ Experimento especÃ­fico con features tÃ¡cticas")
        
        print(f"   5. ğŸ”® Predicciones en tiempo real")
        
    else:
        print("âš ï¸ Dataset unificado no disponible")
        available = [name for name, info in summary.items() if info and info['type'] == 'file']
        if available:
            print(f"ğŸ“Š Usar datasets individuales: {available}")
        else:
            print("âŒ No hay datasets listos para ML")

if __name__ == "__main__":
    summary = analyze_datasets()
    recommend_dataset_strategy(summary)
    
    print(f"\nğŸš€ Siguiente paso: python src/ml/train_basic_model.py")
