# üöÄ GU√çA COMPLETA MLflow para Chess Trainer

## üìã √çNDICE

1. [üèÅ Setup Inicial](#-setup-inicial)
2. [üîÑ Iniciando MLflow](#-iniciando-mlflow)
3. [üìä Carga de Datasets](#-carga-de-datasets)
4. [üéØ Entrenamiento con MLflow](#-entrenamiento-con-mlflow)
5. [üîÆ Predicciones y Registro](#-predicciones-y-registro)
6. [üìà Monitoreo y Evaluaci√≥n](#-monitoreo-y-evaluaci√≥n)
7. [üõ†Ô∏è Troubleshooting](#Ô∏è-troubleshooting)
8. [üéÆ Scripts Automatizados](#-scripts-automatizados)

---

## üèÅ SETUP INICIAL

### Prerequisitos
```bash
# Verificar Docker
docker --version
docker-compose --version

# Verificar Python environment
python --version
pip list | grep mlflow
```

### 1. Configuraci√≥n de Servicios

```bash
# Directorio base
cd c:\Users\sergiosal\source\repos\chess_trainer

# Iniciar servicios base (PostgreSQL)
docker-compose up -d postgres

# Esperar que PostgreSQL arranque
timeout 10

# Iniciar MLflow
docker-compose up -d mlflow

# Verificar servicios
docker-compose ps
```

### 2. Verificar Conectividad

```bash
# Verificar MLflow UI
curl http://localhost:5000/health 2>/dev/null || echo "MLflow no disponible"

# Si no funciona, reiniciar
docker-compose restart mlflow
```

### 3. Estructura de Directorios

```
chess_trainer/
‚îú‚îÄ‚îÄ src/ml/
‚îÇ   ‚îú‚îÄ‚îÄ mlflow_complete_guide.py      # üÜï Gu√≠a unificada
‚îÇ   ‚îú‚îÄ‚îÄ mlflow_utils.py               # Utilidades MLflow
‚îÇ   ‚îî‚îÄ‚îÄ experiments/                  # Experimentos organizados
‚îú‚îÄ‚îÄ models/                           # Modelos entrenados
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ export/                       # Datasets procesados
‚îÇ   ‚îî‚îÄ‚îÄ mlflow_artifacts/             # Artefactos MLflow
‚îî‚îÄ‚îÄ mlruns/                          # Tracking local (backup)
```

---

## üîÑ INICIANDO MLflow

### Configuraci√≥n B√°sica

```python
import mlflow
import mlflow.sklearn
import os
from pathlib import Path

# Configurar MLflow
MLFLOW_TRACKING_URI = "http://localhost:5000"
EXPERIMENT_NAME = "chess_error_prediction"

# Setup
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# Crear experimento si no existe
try:
    mlflow.create_experiment(EXPERIMENT_NAME)
    print(f"‚úÖ Experimento '{EXPERIMENT_NAME}' creado")
except mlflow.exceptions.MlflowException:
    print(f"‚úÖ Usando experimento existente: '{EXPERIMENT_NAME}'")

mlflow.set_experiment(EXPERIMENT_NAME)
```

### Verificaci√≥n de Conexi√≥n

```python
def verify_mlflow_connection():
    """Verificar conexi√≥n con MLflow"""
    try:
        # Test b√°sico
        experiments = mlflow.search_experiments()
        print(f"‚úÖ MLflow conectado - {len(experiments)} experimentos")
        
        # Test de escritura
        with mlflow.start_run(run_name="connection_test"):
            mlflow.log_param("test_param", "test_value")
            mlflow.log_metric("test_metric", 1.0)
            run_id = mlflow.active_run().info.run_id
            print(f"‚úÖ Test run creado: {run_id}")
        
        return True
    except Exception as e:
        print(f"‚ùå Error MLflow: {e}")
        return False

# Ejecutar verificaci√≥n
verify_mlflow_connection()
```

---

## üìä CARGA DE DATASETS

### 1. Ubicaci√≥n de Datasets

```python
import pandas as pd
from pathlib import Path

def find_dataset():
    """Encontrar dataset principal"""
    
    possible_paths = [
        "data/export/unified_small_sources.parquet",
        "data/processed/unified_small_sources.parquet",
        "data/unified_small_sources.parquet"
    ]
    
    for path in possible_paths:
        if Path(path).exists():
            return path
    
    raise FileNotFoundError(f"Dataset no encontrado en: {possible_paths}")

# Encontrar y cargar dataset
dataset_path = find_dataset()
print(f"üìÇ Dataset encontrado: {dataset_path}")
```

### 2. Carga y Exploraci√≥n Inicial

```python
def load_and_explore_dataset(dataset_path):
    """Cargar dataset y exploraci√≥n b√°sica con MLflow logging"""
    
    with mlflow.start_run(run_name="dataset_exploration"):
        
        # Cargar dataset
        df = pd.read_parquet(dataset_path)
        
        # Log informaci√≥n b√°sica
        mlflow.log_param("dataset_path", dataset_path)
        mlflow.log_metric("total_rows", len(df))
        mlflow.log_metric("total_columns", len(df.columns))
        
        # Informaci√≥n sobre error_label (target)
        if 'error_label' in df.columns:
            df_valid = df[df['error_label'].notna()]
            mlflow.log_metric("valid_labels", len(df_valid))
            mlflow.log_metric("valid_percentage", len(df_valid)/len(df)*100)
            
            # Distribuci√≥n de clases
            class_dist = df_valid['error_label'].value_counts()
            for class_name, count in class_dist.items():
                mlflow.log_metric(f"class_count_{class_name}", count)
                mlflow.log_metric(f"class_pct_{class_name}", count/len(df_valid)*100)
            
            print(f"üìä Dataset shape: {df.shape}")
            print(f"üìä Valid labels: {len(df_valid)} ({len(df_valid)/len(df)*100:.1f}%)")
            print(f"üìä Class distribution:")
            for class_name, count in class_dist.items():
                print(f"   ‚Ä¢ {class_name}: {count} ({count/len(df_valid)*100:.1f}%)")
        
        # Log columnas disponibles
        mlflow.log_param("available_columns", list(df.columns))
        
        return df

# Cargar dataset
df = load_and_explore_dataset(dataset_path)
```

### 3. Preparaci√≥n de Features

```python
def prepare_features_for_training(df):
    """Preparar features para entrenamiento"""
    
    with mlflow.start_run(run_name="feature_preparation"):
        
        # Features definidos
        features = [
            'material_balance', 'material_total', 'num_pieces', 
            'branching_factor', 'self_mobility', 'opponent_mobility',
            'score_diff', 'move_number', 'white_elo', 'black_elo'
        ]
        
        # Filtrar datos v√°lidos
        df_valid = df[df['error_label'].notna()].copy()
        
        # Verificar features disponibles
        available_features = [f for f in features if f in df_valid.columns]
        missing_features = [f for f in features if f not in df_valid.columns]
        
        mlflow.log_param("requested_features", features)
        mlflow.log_param("available_features", available_features)
        mlflow.log_param("missing_features", missing_features)
        mlflow.log_metric("feature_coverage", len(available_features)/len(features)*100)
        
        if missing_features:
            print(f"‚ö†Ô∏è  Features faltantes: {missing_features}")
        
        # Preparar X e y
        X = df_valid[available_features].fillna(0)
        y = df_valid['error_label']
        
        # Log estad√≠sticas finales
        mlflow.log_metric("final_samples", len(X))
        mlflow.log_metric("final_features", len(available_features))
        
        # Estad√≠sticas de features
        for feature in available_features:
            mlflow.log_metric(f"feature_mean_{feature}", X[feature].mean())
            mlflow.log_metric(f"feature_std_{feature}", X[feature].std())
            mlflow.log_metric(f"feature_null_pct_{feature}", X[feature].isnull().sum()/len(X)*100)
        
        print(f"‚úÖ Features preparados: {X.shape}")
        print(f"‚úÖ Features disponibles: {len(available_features)}/{len(features)}")
        
        return X, y, available_features

# Preparar features
X, y, features = prepare_features_for_training(df)
```

---

## üéØ ENTRENAMIENTO CON MLflow

### 1. Entrenamiento B√°sico

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

def train_model_with_mlflow(X, y, features):
    """Entrenamiento completo con MLflow tracking"""
    
    with mlflow.start_run(run_name="chess_error_classification_training"):
        
        # Log informaci√≥n del experimento
        mlflow.log_param("algorithm", "RandomForest")
        mlflow.log_param("features_used", features)
        mlflow.log_param("total_samples", len(X))
        
        # Split de datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        mlflow.log_param("train_size", len(X_train))
        mlflow.log_param("test_size", len(X_test))
        mlflow.log_param("test_ratio", 0.2)
        mlflow.log_param("random_state", 42)
        
        # Par√°metros del modelo
        model_params = {
            'n_estimators': 100,
            'random_state': 42,
            'n_jobs': -1,
            'max_depth': None,
            'min_samples_split': 2,
            'min_samples_leaf': 1
        }
        
        # Log par√°metros del modelo
        for param, value in model_params.items():
            mlflow.log_param(f"model_{param}", value)
        
        # Entrenar modelo
        print("üéØ Entrenando RandomForest...")
        import time
        start_time = time.time()
        
        model = RandomForestClassifier(**model_params)
        model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        mlflow.log_metric("training_time_seconds", training_time)
        
        # Predicciones
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        # M√©tricas de accuracy
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        
        mlflow.log_metric("train_accuracy", train_accuracy)
        mlflow.log_metric("test_accuracy", test_accuracy)
        mlflow.log_metric("accuracy_gap", train_accuracy - test_accuracy)
        
        # Reporte detallado por clase
        report = classification_report(y_test, y_test_pred, output_dict=True)
        
        for class_name in ['good', 'inaccuracy', 'mistake', 'blunder']:
            if class_name in report:
                mlflow.log_metric(f"precision_{class_name}", report[class_name]['precision'])
                mlflow.log_metric(f"recall_{class_name}", report[class_name]['recall'])
                mlflow.log_metric(f"f1_{class_name}", report[class_name]['f1-score'])
        
        # M√©tricas agregadas
        mlflow.log_metric("macro_avg_precision", report['macro avg']['precision'])
        mlflow.log_metric("macro_avg_recall", report['macro avg']['recall'])
        mlflow.log_metric("macro_avg_f1", report['macro avg']['f1-score'])
        mlflow.log_metric("weighted_avg_f1", report['weighted avg']['f1-score'])
        
        # Feature importance
        feature_importance = dict(zip(features, model.feature_importances_))
        
        for feature, importance in feature_importance.items():
            mlflow.log_metric(f"importance_{feature}", importance)
        
        # Feature m√°s importante
        most_important_feature = max(feature_importance, key=feature_importance.get)
        mlflow.log_param("most_important_feature", most_important_feature)
        mlflow.log_metric("most_important_score", feature_importance[most_important_feature])
        
        # Guardar modelo en MLflow
        mlflow.sklearn.log_model(
            model, 
            "model",
            registered_model_name="chess_error_classifier",
            input_example=X_test.head(5),
            signature=mlflow.models.infer_signature(X_test, y_test_pred)
        )
        
        # Guardar artefactos adicionales
        
        # 1. Lista de features
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write('\n'.join(features))
            features_file = f.name
        
        mlflow.log_artifact(features_file, "model_artifacts")
        os.unlink(features_file)
        
        # 2. Reporte de clasificaci√≥n
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(classification_report(y_test, y_test_pred))
            report_file = f.name
        
        mlflow.log_artifact(report_file, "evaluation")
        os.unlink(report_file)
        
        # 3. Matriz de confusi√≥n
        cm = confusion_matrix(y_test, y_test_pred)
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            np.savetxt(f.name, cm, delimiter=',', fmt='%d')
            cm_file = f.name
        
        mlflow.log_artifact(cm_file, "evaluation")
        os.unlink(cm_file)
        
        # Log informaci√≥n final
        run_id = mlflow.active_run().info.run_id
        
        print(f"‚úÖ Entrenamiento completado")
        print(f"üéØ Test Accuracy: {test_accuracy:.4f}")
        print(f"‚≠ê Feature m√°s importante: {most_important_feature} ({feature_importance[most_important_feature]:.4f})")
        print(f"üìã MLflow Run ID: {run_id}")
        
        return model, run_id, test_accuracy

# Entrenar modelo
model, run_id, accuracy = train_model_with_mlflow(X, y, features)
```

### 2. Entrenamiento con Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV

def train_with_hyperparameter_tuning(X, y, features):
    """Entrenamiento con optimizaci√≥n de hiperpar√°metros"""
    
    with mlflow.start_run(run_name="hyperparameter_tuning"):
        
        # Split de datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Grid de par√°metros
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        mlflow.log_param("param_grid", str(param_grid))
        mlflow.log_param("cv_folds", 3)
        
        # Grid Search
        print("üîç Ejecutando Grid Search...")
        rf = RandomForestClassifier(random_state=42, n_jobs=-1)
        
        grid_search = GridSearchCV(
            rf, 
            param_grid, 
            cv=3, 
            scoring='f1_weighted',
            n_jobs=-1,
            verbose=1
        )
        
        import time
        start_time = time.time()
        grid_search.fit(X_train, y_train)
        tuning_time = time.time() - start_time
        
        mlflow.log_metric("tuning_time_seconds", tuning_time)
        
        # Mejores par√°metros
        best_params = grid_search.best_params_
        for param, value in best_params.items():
            mlflow.log_param(f"best_{param}", value)
        
        mlflow.log_metric("best_cv_score", grid_search.best_score_)
        
        # Modelo final
        best_model = grid_search.best_estimator_
        
        # Evaluaci√≥n en test
        y_test_pred = best_model.predict(X_test)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        
        mlflow.log_metric("final_test_accuracy", test_accuracy)
        
        # Guardar modelo optimizado
        mlflow.sklearn.log_model(
            best_model,
            "tuned_model",
            registered_model_name="chess_error_classifier_tuned"
        )
        
        run_id = mlflow.active_run().info.run_id
        
        print(f"‚úÖ Tuning completado - Run ID: {run_id}")
        print(f"üéØ Best CV Score: {grid_search.best_score_:.4f}")
        print(f"üéØ Test Accuracy: {test_accuracy:.4f}")
        print(f"‚öôÔ∏è Best params: {best_params}")
        
        return best_model, run_id

# Ejecutar tuning (opcional)
# tuned_model, tuning_run_id = train_with_hyperparameter_tuning(X, y, features)
```

---

## üîÆ PREDICCIONES Y REGISTRO

### 1. Predicciones con Modelo MLflow

```python
def make_predictions_with_mlflow_model(run_id=None, model_name=None):
    """Hacer predicciones usando modelo de MLflow"""
    
    with mlflow.start_run(run_name="model_predictions"):
        
        # Cargar modelo
        if run_id:
            model_uri = f"runs:/{run_id}/model"
            mlflow.log_param("model_source", f"run_id:{run_id}")
        elif model_name:
            model_uri = f"models:/{model_name}/latest"
            mlflow.log_param("model_source", f"registered_model:{model_name}")
        else:
            model_uri = "models:/chess_error_classifier/latest"
            mlflow.log_param("model_source", "latest_registered")
        
        print(f"üì¶ Cargando modelo desde: {model_uri}")
        
        try:
            loaded_model = mlflow.sklearn.load_model(model_uri)
            mlflow.log_param("model_loaded_successfully", True)
        except Exception as e:
            print(f"‚ùå Error cargando modelo: {e}")
            mlflow.log_param("model_load_error", str(e))
            return None
        
        # Cargar datos para predicci√≥n
        X, y, features = prepare_features_for_training(df)
        
        mlflow.log_param("prediction_samples", len(X))
        
        # Hacer predicciones
        print("üîÆ Generando predicciones...")
        start_time = time.time()
        
        predictions = loaded_model.predict(X)
        probabilities = loaded_model.predict_proba(X)
        
        prediction_time = time.time() - start_time
        mlflow.log_metric("prediction_time_seconds", prediction_time)
        mlflow.log_metric("predictions_per_second", len(X) / prediction_time)
        
        # Estad√≠sticas de predicciones
        pred_series = pd.Series(predictions)
        pred_dist = pred_series.value_counts()
        
        for class_name, count in pred_dist.items():
            mlflow.log_metric(f"predicted_count_{class_name}", count)
            mlflow.log_metric(f"predicted_pct_{class_name}", count/len(predictions)*100)
        
        # Estad√≠sticas de confianza
        max_probs = probabilities.max(axis=1)
        
        confidence_stats = {
            "mean_confidence": max_probs.mean(),
            "median_confidence": np.median(max_probs),
            "min_confidence": max_probs.min(),
            "max_confidence": max_probs.max(),
            "high_confidence_count": (max_probs > 0.9).sum(),
            "low_confidence_count": (max_probs < 0.6).sum(),
        }
        
        for metric, value in confidence_stats.items():
            mlflow.log_metric(metric, value)
        
        # Crear DataFrame de resultados
        df_results = df[df['error_label'].notna()].copy()
        df_results['predicted_error'] = predictions
        df_results['prediction_confidence'] = max_probs
        
        # Agregar probabilidades por clase
        classes = loaded_model.classes_
        for i, class_name in enumerate(classes):
            df_results[f'prob_{class_name}'] = probabilities[:, i]
        
        # Guardar resultados
        output_path = "predictions_results_mlflow.parquet"
        df_results.to_parquet(output_path, index=False)
        mlflow.log_artifact(output_path, "predictions")
        
        # Resumen CSV
        summary_path = "predictions_summary_mlflow.csv"
        summary_cols = ['move_san', 'predicted_error', 'prediction_confidence']
        if 'error_label' in df_results.columns:
            summary_cols.append('error_label')
        
        available_cols = [col for col in summary_cols if col in df_results.columns]
        df_summary = df_results[available_cols].head(1000)
        df_summary.to_csv(summary_path, index=False)
        mlflow.log_artifact(summary_path, "predictions")
        
        # Log informaci√≥n final
        prediction_run_id = mlflow.active_run().info.run_id
        
        print(f"‚úÖ Predicciones completadas - Run ID: {prediction_run_id}")
        print(f"üìä {len(predictions)} predicciones generadas")
        print(f"üéØ Confianza promedio: {confidence_stats['mean_confidence']:.3f}")
        print(f"üíæ Resultados guardados en: {output_path}")
        
        return df_results, prediction_run_id

# Hacer predicciones
if 'run_id' in locals():
    results, pred_run_id = make_predictions_with_mlflow_model(run_id=run_id)
```

### 2. Evaluaci√≥n en Datos Reales

```python
def evaluate_model_performance(df_results):
    """Evaluar rendimiento del modelo en datos reales"""
    
    with mlflow.start_run(run_name="model_evaluation"):
        
        # Filtrar datos con etiquetas reales
        df_eval = df_results[df_results['error_label'].notna()]
        
        if len(df_eval) == 0:
            print("‚ö†Ô∏è  No hay datos con etiquetas reales para evaluar")
            return
        
        mlflow.log_metric("evaluation_samples", len(df_eval))
        
        # M√©tricas de evaluaci√≥n
        y_true = df_eval['error_label']
        y_pred = df_eval['predicted_error']
        
        accuracy = accuracy_score(y_true, y_pred)
        mlflow.log_metric("real_data_accuracy", accuracy)
        
        # Reporte por clase
        report = classification_report(y_true, y_pred, output_dict=True)
        
        for class_name in ['good', 'inaccuracy', 'mistake', 'blunder']:
            if class_name in report:
                mlflow.log_metric(f"real_precision_{class_name}", report[class_name]['precision'])
                mlflow.log_metric(f"real_recall_{class_name}", report[class_name]['recall'])
                mlflow.log_metric(f"real_f1_{class_name}", report[class_name]['f1-score'])
        
        # An√°lisis de confianza vs accuracy
        confidence = df_eval['prediction_confidence']
        
        # Accuracy por rangos de confianza
        high_conf = df_eval[confidence > 0.9]
        if len(high_conf) > 0:
            high_conf_acc = accuracy_score(high_conf['error_label'], high_conf['predicted_error'])
            mlflow.log_metric("high_confidence_accuracy", high_conf_acc)
            mlflow.log_metric("high_confidence_samples", len(high_conf))
        
        medium_conf = df_eval[(confidence > 0.7) & (confidence <= 0.9)]
        if len(medium_conf) > 0:
            medium_conf_acc = accuracy_score(medium_conf['error_label'], medium_conf['predicted_error'])
            mlflow.log_metric("medium_confidence_accuracy", medium_conf_acc)
            mlflow.log_metric("medium_confidence_samples", len(medium_conf))
        
        low_conf = df_eval[confidence <= 0.7]
        if len(low_conf) > 0:
            low_conf_acc = accuracy_score(low_conf['error_label'], low_conf['predicted_error'])
            mlflow.log_metric("low_confidence_accuracy", low_conf_acc)
            mlflow.log_metric("low_confidence_samples", len(low_conf))
        
        print(f"‚úÖ Evaluaci√≥n completada")
        print(f"üéØ Accuracy en datos reales: {accuracy:.4f}")
        print(f"üìä Muestras evaluadas: {len(df_eval)}")

# Evaluar si tenemos resultados
if 'results' in locals():
    evaluate_model_performance(results)
```

---

## üìà MONITOREO Y EVALUACI√ìN

### 1. Comparaci√≥n de Experimentos

```python
def compare_experiments():
    """Comparar experimentos en MLflow"""
    
    # Buscar experimentos
    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
    
    if experiment is None:
        print("‚ùå Experimento no encontrado")
        return
    
    # Obtener runs
    runs = mlflow.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=["start_time DESC"],
        max_results=10
    )
    
    if len(runs) == 0:
        print("‚ùå No hay runs en el experimento")
        return
    
    print(f"üìä COMPARACI√ìN DE EXPERIMENTOS")
    print("=" * 60)
    
    # Mostrar m√©tricas clave
    key_metrics = ['test_accuracy', 'macro_avg_f1', 'training_time_seconds']
    
    for metric in key_metrics:
        if metric in runs.columns:
            print(f"\nüéØ {metric.upper()}:")
            top_runs = runs.nlargest(3, metric) if 'accuracy' in metric or 'f1' in metric else runs.nsmallest(3, metric)
            
            for idx, run in top_runs.iterrows():
                run_name = run.get('tags.mlflow.runName', 'N/A')
                value = run[metric] if not pd.isna(run[metric]) else 'N/A'
                print(f"   ‚Ä¢ {run_name}: {value}")
    
    # Mejores runs por m√©trica
    print(f"\nüèÜ MEJORES RUNS:")
    
    if 'test_accuracy' in runs.columns:
        best_accuracy = runs.loc[runs['test_accuracy'].idxmax()]
        print(f"   üéØ Mejor Accuracy: {best_accuracy.get('tags.mlflow.runName', 'N/A')} ({best_accuracy['test_accuracy']:.4f})")
    
    if 'macro_avg_f1' in runs.columns:
        best_f1 = runs.loc[runs['macro_avg_f1'].idxmax()]
        print(f"   üìä Mejor F1: {best_f1.get('tags.mlflow.runName', 'N/A')} ({best_f1['macro_avg_f1']:.4f})")
    
    return runs

# Comparar experimentos
comparison_df = compare_experiments()
```

### 2. An√°lisis de Feature Importance

```python
def analyze_feature_importance():
    """Analizar importancia de features across runs"""
    
    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
    
    # Filtrar runs con feature importance
    importance_cols = [col for col in runs.columns if col.startswith('metrics.importance_')]
    
    if len(importance_cols) == 0:
        print("‚ùå No hay datos de feature importance")
        return
    
    print(f"üìä AN√ÅLISIS DE FEATURE IMPORTANCE")
    print("=" * 50)
    
    # Calcular importancia promedio
    importance_data = runs[importance_cols].mean().sort_values(ascending=False)
    
    print(f"üéØ TOP 5 FEATURES M√ÅS IMPORTANTES:")
    for i, (col, importance) in enumerate(importance_data.head().items(), 1):
        feature_name = col.replace('metrics.importance_', '')
        print(f"   {i}. {feature_name}: {importance:.4f}")
    
    return importance_data

# Analizar feature importance
feature_analysis = analyze_feature_importance()
```

---

## üõ†Ô∏è TROUBLESHOOTING

### Problemas Comunes y Soluciones

```python
def troubleshoot_mlflow():
    """Diagnosticar problemas comunes con MLflow"""
    
    print("üîç DIAGN√ìSTICO MLflow")
    print("=" * 40)
    
    # 1. Verificar conexi√≥n
    try:
        mlflow.search_experiments()
        print("‚úÖ Conexi√≥n MLflow OK")
    except Exception as e:
        print(f"‚ùå Error conexi√≥n: {e}")
        print("üí° Soluci√≥n: docker-compose restart mlflow")
        return
    
    # 2. Verificar experimento
    try:
        experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
        if experiment:
            print(f"‚úÖ Experimento '{EXPERIMENT_NAME}' encontrado")
        else:
            print(f"‚ö†Ô∏è  Experimento '{EXPERIMENT_NAME}' no existe")
            print("üí° Se crear√° autom√°ticamente en el pr√≥ximo run")
    except Exception as e:
        print(f"‚ùå Error experimento: {e}")
    
    # 3. Verificar permisos
    try:
        with mlflow.start_run(run_name="permission_test"):
            mlflow.log_param("test", "value")
        print("‚úÖ Permisos de escritura OK")
    except Exception as e:
        print(f"‚ùå Error permisos: {e}")
        print("üí° Verificar configuraci√≥n Docker")
    
    # 4. Verificar almacenamiento
    try:
        import requests
        response = requests.get(f"{MLFLOW_TRACKING_URI}/health", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Servidor MLflow healthy")
        else:
            print(f"‚ö†Ô∏è  MLflow responde con c√≥digo: {response.status_code}")
    except Exception as e:
        print(f"‚ùå Error servidor: {e}")
        print("üí° Verificar: docker-compose logs mlflow")

# Ejecutar diagn√≥stico
troubleshoot_mlflow()
```

### Comandos de Recuperaci√≥n

```bash
# Si MLflow no responde
docker-compose restart mlflow
docker-compose logs mlflow

# Si hay problemas de base de datos
docker-compose restart postgres
docker-compose exec postgres psql -U postgres -c "\l"

# Reinicio completo
docker-compose down
docker-compose up -d postgres
sleep 10
docker-compose up -d mlflow

# Verificar estado
docker-compose ps
curl http://localhost:5000/health
```

---

## üéÆ SCRIPTS AUTOMATIZADOS

### Script Principal Unificado

```python
def run_complete_mlflow_pipeline():
    """Pipeline completo automatizado"""
    
    print("üöÄ PIPELINE COMPLETO MLflow - CHESS ERROR PREDICTION")
    print("=" * 80)
    
    success_steps = 0
    total_steps = 5
    
    try:
        # PASO 1: Verificar MLflow
        print("\n" + "="*60)
        print("üìã PASO 1/5: Verificando MLflow")
        print("="*60)
        
        if not verify_mlflow_connection():
            print("‚ùå MLflow no disponible. Verifica servicios Docker.")
            return False
        
        success_steps += 1
        
        # PASO 2: Cargar y explorar datos
        print("\n" + "="*60)
        print("üìä PASO 2/5: Cargando y explorando dataset")
        print("="*60)
        
        dataset_path = find_dataset()
        df = load_and_explore_dataset(dataset_path)
        
        success_steps += 1
        
        # PASO 3: Preparar features
        print("\n" + "="*60)
        print("üîß PASO 3/5: Preparando features")
        print("="*60)
        
        X, y, features = prepare_features_for_training(df)
        
        success_steps += 1
        
        # PASO 4: Entrenar modelo
        print("\n" + "="*60)
        print("üéØ PASO 4/5: Entrenando modelo")
        print("="*60)
        
        model, run_id, accuracy = train_model_with_mlflow(X, y, features)
        
        success_steps += 1
        
        # PASO 5: Hacer predicciones
        print("\n" + "="*60)
        print("üîÆ PASO 5/5: Generando predicciones")
        print("="*60)
        
        results, pred_run_id = make_predictions_with_mlflow_model(run_id=run_id)
        evaluate_model_performance(results)
        
        success_steps += 1
        
        # RESUMEN FINAL
        print("\n" + "="*80)
        print("üéâ PIPELINE COMPLETADO EXITOSAMENTE")
        print("="*80)
        
        print(f"‚úÖ Pasos completados: {success_steps}/{total_steps}")
        print(f"üéØ Accuracy obtenida: {accuracy:.4f}")
        print(f"üìä Muestras procesadas: {len(results)}")
        
        print(f"\nüåê ENLACES MLflow:")
        print(f"   ‚Ä¢ UI Principal: http://localhost:5000")
        print(f"   ‚Ä¢ Experimento: http://localhost:5000/#/experiments")
        print(f"   ‚Ä¢ Training Run: http://localhost:5000/#/experiments/1/runs/{run_id}")
        print(f"   ‚Ä¢ Prediction Run: http://localhost:5000/#/experiments/1/runs/{pred_run_id}")
        
        print(f"\nüíæ ARCHIVOS GENERADOS:")
        print("   ‚Ä¢ predictions_results_mlflow.parquet")
        print("   ‚Ä¢ predictions_summary_mlflow.csv")
        print("   ‚Ä¢ Modelo registrado en MLflow")
        
        print(f"\nüöÄ PR√ìXIMOS PASOS:")
        print("   1. üîç Explorar resultados en MLflow UI")
        print("   2. üìä Comparar experimentos")
        print("   3. üéØ Optimizar hiperpar√°metros")
        print("   4. üîÆ Integrar en aplicaci√≥n")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå ERROR EN PASO {success_steps + 1}/{total_steps}")
        print(f"Error: {e}")
        print(f"\nüîç Para diagnosticar:")
        print("troubleshoot_mlflow()")
        return False

# EJECUTAR PIPELINE COMPLETO
if __name__ == "__main__":
    success = run_complete_mlflow_pipeline()
    
    if success:
        print(f"\nüéØ ¬°TUTORIAL MLflow COMPLETADO!")
        print("Revisa la UI de MLflow en: http://localhost:5000")
    else:
        print(f"\n‚ö†Ô∏è  Pipeline incompleto. Revisa los errores anteriores.")
```

### Comandos de Uso R√°pido

```python
# COMANDOS R√ÅPIDOS PARA COPIAR/PEGAR

# 1. Setup inicial
"""
# Terminal
docker-compose up -d postgres mlflow
timeout 10
"""

# 2. Verificaci√≥n r√°pida
"""
verify_mlflow_connection()
"""

# 3. Pipeline completo
"""
run_complete_mlflow_pipeline()
"""

# 4. Solo entrenamiento
"""
dataset_path = find_dataset()
df = load_and_explore_dataset(dataset_path)
X, y, features = prepare_features_for_training(df)
model, run_id, accuracy = train_model_with_mlflow(X, y, features)
"""

# 5. Solo predicciones
"""
results, pred_run_id = make_predictions_with_mlflow_model(run_id="TU_RUN_ID")
"""

# 6. Comparar experimentos
"""
comparison_df = compare_experiments()
feature_analysis = analyze_feature_importance()
"""

# 7. Diagn√≥stico
"""
troubleshoot_mlflow()
"""
```

---

## üéØ RESULTADO ESPERADO

Al final de esta gu√≠a tendr√°s:

‚úÖ **MLflow funcionando** con tracking completo
‚úÖ **Modelo entrenado** con m√©tricas registradas  
‚úÖ **Predicciones generadas** con evaluaci√≥n
‚úÖ **Experimentos comparables** en UI web
‚úÖ **Artefactos guardados** (modelo, reportes, datos)
‚úÖ **Pipeline reproducible** para futuras iteraciones

**üåê Accede a MLflow UI**: http://localhost:5000

**üìä Revisa tus experimentos, m√©tricas, y modelos registrados!**

---

*Gu√≠a creada: Julio 2025 | Chess Trainer ML Pipeline*
