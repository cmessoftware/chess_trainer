# ğŸš€ GUÃA COMPLETA MLflow para Chess Trainer

## ğŸ“‹ ÃNDICE

1. [ğŸ Setup Inicial](#-setup-inicial)
2. [ğŸ”„ Iniciando MLflow](#-iniciando-mlflow)
3. [ğŸ“Š Carga de Datasets](#-carga-de-datasets)
4. [ğŸ¯ Entrenamiento con MLflow](#-entrenamiento-con-mlflow)
5. [ğŸ”® Predicciones y Registro](#-predicciones-y-registro)
6. [ğŸ“ˆ Monitoreo y EvaluaciÃ³n](#-monitoreo-y-evaluaciÃ³n)
7. [ğŸ› ï¸ Troubleshooting](#ï¸-troubleshooting)
8. [ğŸ® Scripts Automatizados](#-scripts-automatizados)

---

## ğŸ SETUP INICIAL

### Prerequisitos
```bash
# Verificar Docker
docker --version
docker-compose --version

# Verificar Python environment
python --version
pip list | grep mlflow
```

### 1. ConfiguraciÃ³n de Servicios

```bash
# Directorio base
cd c:\Users\sergiosal\source\repos\chess_trainer

# Iniciar servicios base (PostgreSQL)
docker-compose up -d postgres

# Esperar que PostgreSQL arranque
timeout 10

# Iniciar MLflow
docker-compose up -d mlflow

# Verificar servicios
docker-compose ps
```

### 2. Verificar Conectividad

```bash
# Verificar MLflow UI
curl http://localhost:5000/health 2>/dev/null || echo "MLflow no disponible"

# Si no funciona, reiniciar
docker-compose restart mlflow
```

### 3. Estructura de Directorios

```
chess_trainer/
â”œâ”€â”€ src/ml/
â”‚   â”œâ”€â”€ mlflow_complete_guide.py      # ğŸ†• GuÃ­a unificada
â”‚   â”œâ”€â”€ mlflow_utils.py               # Utilidades MLflow
â”‚   â””â”€â”€ experiments/                  # Experimentos organizados
â”œâ”€â”€ models/                           # Modelos entrenados
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ export/                       # Datasets procesados
â”‚   â””â”€â”€ mlflow_artifacts/             # Artefactos MLflow
â””â”€â”€ mlruns/                          # Tracking local (backup)
```

---

## ğŸ”„ INICIANDO MLflow

### ConfiguraciÃ³n BÃ¡sica

```python
import mlflow
import mlflow.sklearn
import os
from pathlib import Path

# Configurar MLflow
MLFLOW_TRACKING_URI = "http://localhost:5000"
EXPERIMENT_NAME = "chess_error_prediction"

# Setup
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# Crear experimento si no existe
try:
    mlflow.create_experiment(EXPERIMENT_NAME)
    print(f"âœ… Experimento '{EXPERIMENT_NAME}' creado")
except mlflow.exceptions.MlflowException:
    print(f"âœ… Usando experimento existente: '{EXPERIMENT_NAME}'")

mlflow.set_experiment(EXPERIMENT_NAME)
```

### VerificaciÃ³n de ConexiÃ³n

```python
def verify_mlflow_connection():
    """Verificar conexiÃ³n con MLflow"""
    try:
        # Test bÃ¡sico
        experiments = mlflow.search_experiments()
        print(f"âœ… MLflow conectado - {len(experiments)} experimentos")
        
        # Test de escritura
        with mlflow.start_run(run_name="connection_test"):
            mlflow.log_param("test_param", "test_value")
            mlflow.log_metric("test_metric", 1.0)
            run_id = mlflow.active_run().info.run_id
            print(f"âœ… Test run creado: {run_id}")
        
        return True
    except Exception as e:
        print(f"âŒ Error MLflow: {e}")
        return False

# Ejecutar verificaciÃ³n
verify_mlflow_connection()
```

---

## ğŸ“Š CARGA DE DATASETS

### 1. UbicaciÃ³n de Datasets

```python
import pandas as pd
from pathlib import Path

def find_dataset():
    """Encontrar dataset principal"""
    
    possible_paths = [
        "data/export/unified_small_sources.parquet",
        "data/processed/unified_small_sources.parquet",
        "data/unified_small_sources.parquet"
    ]
    
    for path in possible_paths:
        if Path(path).exists():
            return path
    
    raise FileNotFoundError(f"Dataset no encontrado en: {possible_paths}")

# Encontrar y cargar dataset
dataset_path = find_dataset()
print(f"ğŸ“‚ Dataset encontrado: {dataset_path}")
```

### 2. Carga y ExploraciÃ³n Inicial

```python
def load_and_explore_dataset(dataset_path):
    """Cargar dataset y exploraciÃ³n bÃ¡sica con MLflow logging"""
    
    with mlflow.start_run(run_name="dataset_exploration"):
        
        # Cargar dataset
        df = pd.read_parquet(dataset_path)
        
        # Log informaciÃ³n bÃ¡sica
        mlflow.log_param("dataset_path", dataset_path)
        mlflow.log_metric("total_rows", len(df))
        mlflow.log_metric("total_columns", len(df.columns))
        
        # InformaciÃ³n sobre error_label (target)
        if 'error_label' in df.columns:
            df_valid = df[df['error_label'].notna()]
            mlflow.log_metric("valid_labels", len(df_valid))
            mlflow.log_metric("valid_percentage", len(df_valid)/len(df)*100)
            
            # DistribuciÃ³n de clases
            class_dist = df_valid['error_label'].value_counts()
            for class_name, count in class_dist.items():
                mlflow.log_metric(f"class_count_{class_name}", count)
                mlflow.log_metric(f"class_pct_{class_name}", count/len(df_valid)*100)
            
            print(f"ğŸ“Š Dataset shape: {df.shape}")
            print(f"ğŸ“Š Valid labels: {len(df_valid)} ({len(df_valid)/len(df)*100:.1f}%)")
            print(f"ğŸ“Š Class distribution:")
            for class_name, count in class_dist.items():
                print(f"   â€¢ {class_name}: {count} ({count/len(df_valid)*100:.1f}%)")
        
        # Log columnas disponibles
        mlflow.log_param("available_columns", list(df.columns))
        
        return df

# Cargar dataset
df = load_and_explore_dataset(dataset_path)
```

### 3. PreparaciÃ³n de Features

```python
def prepare_features_for_training(df):
    """Preparar features para entrenamiento"""
    
    with mlflow.start_run(run_name="feature_preparation"):
        
        # Features definidos
        features = [
            'material_balance', 'material_total', 'num_pieces', 
            'branching_factor', 'self_mobility', 'opponent_mobility',
            'score_diff', 'move_number', 'white_elo', 'black_elo'
        ]
        
        # Filtrar datos vÃ¡lidos
        df_valid = df[df['error_label'].notna()].copy()
        
        # Verificar features disponibles
        available_features = [f for f in features if f in df_valid.columns]
        missing_features = [f for f in features if f not in df_valid.columns]
        
        mlflow.log_param("requested_features", features)
        mlflow.log_param("available_features", available_features)
        mlflow.log_param("missing_features", missing_features)
        mlflow.log_metric("feature_coverage", len(available_features)/len(features)*100)
        
        if missing_features:
            print(f"âš ï¸  Features faltantes: {missing_features}")
        
        # Preparar X e y
        X = df_valid[available_features].fillna(0)
        y = df_valid['error_label']
        
        # Log estadÃ­sticas finales
        mlflow.log_metric("final_samples", len(X))
        mlflow.log_metric("final_features", len(available_features))
        
        # EstadÃ­sticas de features
        for feature in available_features:
            mlflow.log_metric(f"feature_mean_{feature}", X[feature].mean())
            mlflow.log_metric(f"feature_std_{feature}", X[feature].std())
            mlflow.log_metric(f"feature_null_pct_{feature}", X[feature].isnull().sum()/len(X)*100)
        
        print(f"âœ… Features preparados: {X.shape}")
        print(f"âœ… Features disponibles: {len(available_features)}/{len(features)}")
        
        return X, y, available_features

# Preparar features
X, y, features = prepare_features_for_training(df)
```

---

## ğŸ¯ ENTRENAMIENTO CON MLflow

### 1. Entrenamiento BÃ¡sico

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

def train_model_with_mlflow(X, y, features):
    """Entrenamiento completo con MLflow tracking"""
    
    with mlflow.start_run(run_name="chess_error_classification_training"):
        
        # Log informaciÃ³n del experimento
        mlflow.log_param("algorithm", "RandomForest")
        mlflow.log_param("features_used", features)
        mlflow.log_param("total_samples", len(X))
        
        # Split de datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        mlflow.log_param("train_size", len(X_train))
        mlflow.log_param("test_size", len(X_test))
        mlflow.log_param("test_ratio", 0.2)
        mlflow.log_param("random_state", 42)
        
        # ParÃ¡metros del modelo
        model_params = {
            'n_estimators': 100,
            'random_state': 42,
            'n_jobs': -1,
            'max_depth': None,
            'min_samples_split': 2,
            'min_samples_leaf': 1
        }
        
        # Log parÃ¡metros del modelo
        for param, value in model_params.items():
            mlflow.log_param(f"model_{param}", value)
        
        # Entrenar modelo
        print("ğŸ¯ Entrenando RandomForest...")
        import time
        start_time = time.time()
        
        model = RandomForestClassifier(**model_params)
        model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        mlflow.log_metric("training_time_seconds", training_time)
        
        # Predicciones
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        # MÃ©tricas de accuracy
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        
        mlflow.log_metric("train_accuracy", train_accuracy)
        mlflow.log_metric("test_accuracy", test_accuracy)
        mlflow.log_metric("accuracy_gap", train_accuracy - test_accuracy)
        
        # Reporte detallado por clase
        report = classification_report(y_test, y_test_pred, output_dict=True)
        
        for class_name in ['good', 'inaccuracy', 'mistake', 'blunder']:
            if class_name in report:
                mlflow.log_metric(f"precision_{class_name}", report[class_name]['precision'])
                mlflow.log_metric(f"recall_{class_name}", report[class_name]['recall'])
                mlflow.log_metric(f"f1_{class_name}", report[class_name]['f1-score'])
        
        # MÃ©tricas agregadas
        mlflow.log_metric("macro_avg_precision", report['macro avg']['precision'])
        mlflow.log_metric("macro_avg_recall", report['macro avg']['recall'])
        mlflow.log_metric("macro_avg_f1", report['macro avg']['f1-score'])
        mlflow.log_metric("weighted_avg_f1", report['weighted avg']['f1-score'])
        
        # Feature importance
        feature_importance = dict(zip(features, model.feature_importances_))
        
        for feature, importance in feature_importance.items():
            mlflow.log_metric(f"importance_{feature}", importance)
        
        # Feature mÃ¡s importante
        most_important_feature = max(feature_importance, key=feature_importance.get)
        mlflow.log_param("most_important_feature", most_important_feature)
        mlflow.log_metric("most_important_score", feature_importance[most_important_feature])
        
        # Guardar modelo en MLflow
        mlflow.sklearn.log_model(
            model, 
            "model",
            registered_model_name="chess_error_classifier",
            input_example=X_test.head(5),
            signature=mlflow.models.infer_signature(X_test, y_test_pred)
        )
        
        # Guardar artefactos adicionales
        
        # 1. Lista de features
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write('\n'.join(features))
            features_file = f.name
        
        mlflow.log_artifact(features_file, "model_artifacts")
        os.unlink(features_file)
        
        # 2. Reporte de clasificaciÃ³n
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(classification_report(y_test, y_test_pred))
            report_file = f.name
        
        mlflow.log_artifact(report_file, "evaluation")
        os.unlink(report_file)
        
        # 3. Matriz de confusiÃ³n
        cm = confusion_matrix(y_test, y_test_pred)
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            np.savetxt(f.name, cm, delimiter=',', fmt='%d')
            cm_file = f.name
        
        mlflow.log_artifact(cm_file, "evaluation")
        os.unlink(cm_file)
        
        # Log informaciÃ³n final
        run_id = mlflow.active_run().info.run_id
        
        print(f"âœ… Entrenamiento completado")
        print(f"ğŸ¯ Test Accuracy: {test_accuracy:.4f}")
        print(f"â­ Feature mÃ¡s importante: {most_important_feature} ({feature_importance[most_important_feature]:.4f})")
        print(f"ğŸ“‹ MLflow Run ID: {run_id}")
        
        return model, run_id, test_accuracy

# Entrenar modelo
model, run_id, accuracy = train_model_with_mlflow(X, y, features)
```

### 2. Entrenamiento con Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV

def train_with_hyperparameter_tuning(X, y, features):
    """Entrenamiento con optimizaciÃ³n de hiperparÃ¡metros"""
    
    with mlflow.start_run(run_name="hyperparameter_tuning"):
        
        # Split de datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Grid de parÃ¡metros
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        mlflow.log_param("param_grid", str(param_grid))
        mlflow.log_param("cv_folds", 3)
        
        # Grid Search
        print("ğŸ” Ejecutando Grid Search...")
        rf = RandomForestClassifier(random_state=42, n_jobs=-1)
        
        grid_search = GridSearchCV(
            rf, 
            param_grid, 
            cv=3, 
            scoring='f1_weighted',
            n_jobs=-1,
            verbose=1
        )
        
        import time
        start_time = time.time()
        grid_search.fit(X_train, y_train)
        tuning_time = time.time() - start_time
        
        mlflow.log_metric("tuning_time_seconds", tuning_time)
        
        # Mejores parÃ¡metros
        best_params = grid_search.best_params_
        for param, value in best_params.items():
            mlflow.log_param(f"best_{param}", value)
        
        mlflow.log_metric("best_cv_score", grid_search.best_score_)
        
        # Modelo final
        best_model = grid_search.best_estimator_
        
        # EvaluaciÃ³n en test
        y_test_pred = best_model.predict(X_test)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        
        mlflow.log_metric("final_test_accuracy", test_accuracy)
        
        # Guardar modelo optimizado
        mlflow.sklearn.log_model(
            best_model,
            "tuned_model",
            registered_model_name="chess_error_classifier_tuned"
        )
        
        run_id = mlflow.active_run().info.run_id
        
        print(f"âœ… Tuning completado - Run ID: {run_id}")
        print(f"ğŸ¯ Best CV Score: {grid_search.best_score_:.4f}")
        print(f"ğŸ¯ Test Accuracy: {test_accuracy:.4f}")
        print(f"âš™ï¸ Best params: {best_params}")
        
        return best_model, run_id

# Ejecutar tuning (opcional)
# tuned_model, tuning_run_id = train_with_hyperparameter_tuning(X, y, features)
```

---

## ğŸ”® PREDICCIONES Y REGISTRO

### 1. Predicciones con Modelo MLflow

```python
def make_predictions_with_mlflow_model(run_id=None, model_name=None):
    """Hacer predicciones usando modelo de MLflow"""
    
    with mlflow.start_run(run_name="model_predictions"):
        
        # Cargar modelo
        if run_id:
            model_uri = f"runs:/{run_id}/model"
            mlflow.log_param("model_source", f"run_id:{run_id}")
        elif model_name:
            model_uri = f"models:/{model_name}/latest"
            mlflow.log_param("model_source", f"registered_model:{model_name}")
        else:
            model_uri = "models:/chess_error_classifier/latest"
            mlflow.log_param("model_source", "latest_registered")
        
        print(f"ğŸ“¦ Cargando modelo desde: {model_uri}")
        
        try:
            loaded_model = mlflow.sklearn.load_model(model_uri)
            mlflow.log_param("model_loaded_successfully", True)
        except Exception as e:
            print(f"âŒ Error cargando modelo: {e}")
            mlflow.log_param("model_load_error", str(e))
            return None
        
        # Cargar datos para predicciÃ³n
        X, y, features = prepare_features_for_training(df)
        
        mlflow.log_param("prediction_samples", len(X))
        
        # Hacer predicciones
        print("ğŸ”® Generando predicciones...")
        start_time = time.time()
        
        predictions = loaded_model.predict(X)
        probabilities = loaded_model.predict_proba(X)
        
        prediction_time = time.time() - start_time
        mlflow.log_metric("prediction_time_seconds", prediction_time)
        mlflow.log_metric("predictions_per_second", len(X) / prediction_time)
        
        # EstadÃ­sticas de predicciones
        pred_series = pd.Series(predictions)
        pred_dist = pred_series.value_counts()
        
        for class_name, count in pred_dist.items():
            mlflow.log_metric(f"predicted_count_{class_name}", count)
            mlflow.log_metric(f"predicted_pct_{class_name}", count/len(predictions)*100)
        
        # EstadÃ­sticas de confianza
        max_probs = probabilities.max(axis=1)
        
        confidence_stats = {
            "mean_confidence": max_probs.mean(),
            "median_confidence": np.median(max_probs),
            "min_confidence": max_probs.min(),
            "max_confidence": max_probs.max(),
            "high_confidence_count": (max_probs > 0.9).sum(),
            "low_confidence_count": (max_probs < 0.6).sum(),
        }
        
        for metric, value in confidence_stats.items():
            mlflow.log_metric(metric, value)
        
        # Crear DataFrame de resultados
        df_results = df[df['error_label'].notna()].copy()
        df_results['predicted_error'] = predictions
        df_results['prediction_confidence'] = max_probs
        
        # Agregar probabilidades por clase
        classes = loaded_model.classes_
        for i, class_name in enumerate(classes):
            df_results[f'prob_{class_name}'] = probabilities[:, i]
        
        # Guardar resultados
        output_path = "predictions_results_mlflow.parquet"
        df_results.to_parquet(output_path, index=False)
        mlflow.log_artifact(output_path, "predictions")
        
        # Resumen CSV
        summary_path = "predictions_summary_mlflow.csv"
        summary_cols = ['move_san', 'predicted_error', 'prediction_confidence']
        if 'error_label' in df_results.columns:
            summary_cols.append('error_label')
        
        available_cols = [col for col in summary_cols if col in df_results.columns]
        df_summary = df_results[available_cols].head(1000)
        df_summary.to_csv(summary_path, index=False)
        mlflow.log_artifact(summary_path, "predictions")
        
        # Log informaciÃ³n final
        prediction_run_id = mlflow.active_run().info.run_id
        
        print(f"âœ… Predicciones completadas - Run ID: {prediction_run_id}")
        print(f"ğŸ“Š {len(predictions)} predicciones generadas")
        print(f"ğŸ¯ Confianza promedio: {confidence_stats['mean_confidence']:.3f}")
        print(f"ğŸ’¾ Resultados guardados en: {output_path}")
        
        return df_results, prediction_run_id

# Hacer predicciones
if 'run_id' in locals():
    results, pred_run_id = make_predictions_with_mlflow_model(run_id=run_id)
```

### 2. EvaluaciÃ³n en Datos Reales

```python
def evaluate_model_performance(df_results):
    """Evaluar rendimiento del modelo en datos reales"""
    
    with mlflow.start_run(run_name="model_evaluation"):
        
        # Filtrar datos con etiquetas reales
        df_eval = df_results[df_results['error_label'].notna()]
        
        if len(df_eval) == 0:
            print("âš ï¸  No hay datos con etiquetas reales para evaluar")
            return
        
        mlflow.log_metric("evaluation_samples", len(df_eval))
        
        # MÃ©tricas de evaluaciÃ³n
        y_true = df_eval['error_label']
        y_pred = df_eval['predicted_error']
        
        accuracy = accuracy_score(y_true, y_pred)
        mlflow.log_metric("real_data_accuracy", accuracy)
        
        # Reporte por clase
        report = classification_report(y_true, y_pred, output_dict=True)
        
        for class_name in ['good', 'inaccuracy', 'mistake', 'blunder']:
            if class_name in report:
                mlflow.log_metric(f"real_precision_{class_name}", report[class_name]['precision'])
                mlflow.log_metric(f"real_recall_{class_name}", report[class_name]['recall'])
                mlflow.log_metric(f"real_f1_{class_name}", report[class_name]['f1-score'])
        
        # AnÃ¡lisis de confianza vs accuracy
        confidence = df_eval['prediction_confidence']
        
        # Accuracy por rangos de confianza
        high_conf = df_eval[confidence > 0.9]
        if len(high_conf) > 0:
            high_conf_acc = accuracy_score(high_conf['error_label'], high_conf['predicted_error'])
            mlflow.log_metric("high_confidence_accuracy", high_conf_acc)
            mlflow.log_metric("high_confidence_samples", len(high_conf))
        
        medium_conf = df_eval[(confidence > 0.7) & (confidence <= 0.9)]
        if len(medium_conf) > 0:
            medium_conf_acc = accuracy_score(medium_conf['error_label'], medium_conf['predicted_error'])
            mlflow.log_metric("medium_confidence_accuracy", medium_conf_acc)
            mlflow.log_metric("medium_confidence_samples", len(medium_conf))
        
        low_conf = df_eval[confidence <= 0.7]
        if len(low_conf) > 0:
            low_conf_acc = accuracy_score(low_conf['error_label'], low_conf['predicted_error'])
            mlflow.log_metric("low_confidence_accuracy", low_conf_acc)
            mlflow.log_metric("low_confidence_samples", len(low_conf))
        
        print(f"âœ… EvaluaciÃ³n completada")
        print(f"ğŸ¯ Accuracy en datos reales: {accuracy:.4f}")
        print(f"ğŸ“Š Muestras evaluadas: {len(df_eval)}")

# Evaluar si tenemos resultados
if 'results' in locals():
    evaluate_model_performance(results)
```

---

## ğŸ“ˆ MONITOREO Y EVALUACIÃ“N

### 1. ComparaciÃ³n de Experimentos

```python
def compare_experiments():
    """Comparar experimentos en MLflow"""
    
    # Buscar experimentos
    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
    
    if experiment is None:
        print("âŒ Experimento no encontrado")
        return
    
    # Obtener runs
    runs = mlflow.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=["start_time DESC"],
        max_results=10
    )
    
    if len(runs) == 0:
        print("âŒ No hay runs en el experimento")
        return
    
    print(f"ğŸ“Š COMPARACIÃ“N DE EXPERIMENTOS")
    print("=" * 60)
    
    # Mostrar mÃ©tricas clave
    key_metrics = ['test_accuracy', 'macro_avg_f1', 'training_time_seconds']
    
    for metric in key_metrics:
        if metric in runs.columns:
            print(f"\nğŸ¯ {metric.upper()}:")
            top_runs = runs.nlargest(3, metric) if 'accuracy' in metric or 'f1' in metric else runs.nsmallest(3, metric)
            
            for idx, run in top_runs.iterrows():
                run_name = run.get('tags.mlflow.runName', 'N/A')
                value = run[metric] if not pd.isna(run[metric]) else 'N/A'
                print(f"   â€¢ {run_name}: {value}")
    
    # Mejores runs por mÃ©trica
    print(f"\nğŸ† MEJORES RUNS:")
    
    if 'test_accuracy' in runs.columns:
        best_accuracy = runs.loc[runs['test_accuracy'].idxmax()]
        print(f"   ğŸ¯ Mejor Accuracy: {best_accuracy.get('tags.mlflow.runName', 'N/A')} ({best_accuracy['test_accuracy']:.4f})")
    
    if 'macro_avg_f1' in runs.columns:
        best_f1 = runs.loc[runs['macro_avg_f1'].idxmax()]
        print(f"   ğŸ“Š Mejor F1: {best_f1.get('tags.mlflow.runName', 'N/A')} ({best_f1['macro_avg_f1']:.4f})")
    
    return runs

# Comparar experimentos
comparison_df = compare_experiments()
```

### 2. AnÃ¡lisis de Feature Importance

```python
def analyze_feature_importance():
    """Analizar importancia de features across runs"""
    
    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
    
    # Filtrar runs con feature importance
    importance_cols = [col for col in runs.columns if col.startswith('metrics.importance_')]
    
    if len(importance_cols) == 0:
        print("âŒ No hay datos de feature importance")
        return
    
    print(f"ğŸ“Š ANÃLISIS DE FEATURE IMPORTANCE")
    print("=" * 50)
    
    # Calcular importancia promedio
    importance_data = runs[importance_cols].mean().sort_values(ascending=False)
    
    print(f"ğŸ¯ TOP 5 FEATURES MÃS IMPORTANTES:")
    for i, (col, importance) in enumerate(importance_data.head().items(), 1):
        feature_name = col.replace('metrics.importance_', '')
        print(f"   {i}. {feature_name}: {importance:.4f}")
    
    return importance_data

# Analizar feature importance
feature_analysis = analyze_feature_importance()
```

---

## ğŸ› ï¸ TROUBLESHOOTING

### Problemas Comunes y Soluciones

```python
def troubleshoot_mlflow():
    """Diagnosticar problemas comunes con MLflow"""
    
    print("ğŸ” DIAGNÃ“STICO MLflow")
    print("=" * 40)
    
    # 1. Verificar conexiÃ³n
    try:
        mlflow.search_experiments()
        print("âœ… ConexiÃ³n MLflow OK")
    except Exception as e:
        print(f"âŒ Error conexiÃ³n: {e}")
        print("ğŸ’¡ SoluciÃ³n: docker-compose restart mlflow")
        return
    
    # 2. Verificar experimento
    try:
        experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
        if experiment:
            print(f"âœ… Experimento '{EXPERIMENT_NAME}' encontrado")
        else:
            print(f"âš ï¸  Experimento '{EXPERIMENT_NAME}' no existe")
            print("ğŸ’¡ Se crearÃ¡ automÃ¡ticamente en el prÃ³ximo run")
    except Exception as e:
        print(f"âŒ Error experimento: {e}")
    
    # 3. Verificar permisos
    try:
        with mlflow.start_run(run_name="permission_test"):
            mlflow.log_param("test", "value")
        print("âœ… Permisos de escritura OK")
    except Exception as e:
        print(f"âŒ Error permisos: {e}")
        print("ğŸ’¡ Verificar configuraciÃ³n Docker")
    
    # 4. Verificar almacenamiento
    try:
        import requests
        response = requests.get(f"{MLFLOW_TRACKING_URI}/health", timeout=5)
        if response.status_code == 200:
            print("âœ… Servidor MLflow healthy")
        else:
            print(f"âš ï¸  MLflow responde con cÃ³digo: {response.status_code}")
    except Exception as e:
        print(f"âŒ Error servidor: {e}")
        print("ğŸ’¡ Verificar: docker-compose logs mlflow")

# Ejecutar diagnÃ³stico
troubleshoot_mlflow()
```

### Comandos de RecuperaciÃ³n

```bash
# Si MLflow no responde
docker-compose restart mlflow
docker-compose logs mlflow

# Si hay problemas de base de datos
docker-compose restart postgres
docker-compose exec postgres psql -U postgres -c "\l"

# Reinicio completo
docker-compose down
docker-compose up -d postgres
sleep 10
docker-compose up -d mlflow

# Verificar estado
docker-compose ps
curl http://localhost:5000/health
```

---

## ğŸ® SCRIPTS AUTOMATIZADOS

### Script Principal Unificado

```python
def run_complete_mlflow_pipeline():
    """Pipeline completo automatizado"""
    
    print("ğŸš€ PIPELINE COMPLETO MLflow - CHESS ERROR PREDICTION")
    print("=" * 80)
    
    success_steps = 0
    total_steps = 5
    
    try:
        # PASO 1: Verificar MLflow
        print("\n" + "="*60)
        print("ğŸ“‹ PASO 1/5: Verificando MLflow")
        print("="*60)
        
        if not verify_mlflow_connection():
            print("âŒ MLflow no disponible. Verifica servicios Docker.")
            return False
        
        success_steps += 1
        
        # PASO 2: Cargar y explorar datos
        print("\n" + "="*60)
        print("ğŸ“Š PASO 2/5: Cargando y explorando dataset")
        print("="*60)
        
        dataset_path = find_dataset()
        df = load_and_explore_dataset(dataset_path)
        
        success_steps += 1
        
        # PASO 3: Preparar features
        print("\n" + "="*60)
        print("ğŸ”§ PASO 3/5: Preparando features")
        print("="*60)
        
        X, y, features = prepare_features_for_training(df)
        
        success_steps += 1
        
        # PASO 4: Entrenar modelo
        print("\n" + "="*60)
        print("ğŸ¯ PASO 4/5: Entrenando modelo")
        print("="*60)
        
        model, run_id, accuracy = train_model_with_mlflow(X, y, features)
        
        success_steps += 1
        
        # PASO 5: Hacer predicciones
        print("\n" + "="*60)
        print("ğŸ”® PASO 5/5: Generando predicciones")
        print("="*60)
        
        results, pred_run_id = make_predictions_with_mlflow_model(run_id=run_id)
        evaluate_model_performance(results)
        
        success_steps += 1
        
        # RESUMEN FINAL
        print("\n" + "="*80)
        print("ğŸ‰ PIPELINE COMPLETADO EXITOSAMENTE")
        print("="*80)
        
        print(f"âœ… Pasos completados: {success_steps}/{total_steps}")
        print(f"ğŸ¯ Accuracy obtenida: {accuracy:.4f}")
        print(f"ğŸ“Š Muestras procesadas: {len(results)}")
        
        print(f"\nğŸŒ ENLACES MLflow:")
        print(f"   â€¢ UI Principal: http://localhost:5000")
        print(f"   â€¢ Experimento: http://localhost:5000/#/experiments")
        print(f"   â€¢ Training Run: http://localhost:5000/#/experiments/1/runs/{run_id}")
        print(f"   â€¢ Prediction Run: http://localhost:5000/#/experiments/1/runs/{pred_run_id}")
        
        print(f"\nğŸ’¾ ARCHIVOS GENERADOS:")
        print("   â€¢ predictions_results_mlflow.parquet")
        print("   â€¢ predictions_summary_mlflow.csv")
        print("   â€¢ Modelo registrado en MLflow")
        
        print(f"\nğŸš€ PRÃ“XIMOS PASOS:")
        print("   1. ğŸ” Explorar resultados en MLflow UI")
        print("   2. ğŸ“Š Comparar experimentos")
        print("   3. ğŸ¯ Optimizar hiperparÃ¡metros")
        print("   4. ğŸ”® Integrar en aplicaciÃ³n")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ERROR EN PASO {success_steps + 1}/{total_steps}")
        print(f"Error: {e}")
        print(f"\nğŸ” Para diagnosticar:")
        print("troubleshoot_mlflow()")
        return False

# EJECUTAR PIPELINE COMPLETO
if __name__ == "__main__":
    success = run_complete_mlflow_pipeline()
    
    if success:
        print(f"\nğŸ¯ Â¡TUTORIAL MLflow COMPLETADO!")
        print("Revisa la UI de MLflow en: http://localhost:5000")
    else:
        print(f"\nâš ï¸  Pipeline incompleto. Revisa los errores anteriores.")
```

### Comandos de Uso RÃ¡pido

```python
# COMANDOS RÃPIDOS PARA COPIAR/PEGAR

# 1. Setup inicial
"""
# Terminal
docker-compose up -d postgres mlflow
timeout 10
"""

# 2. VerificaciÃ³n rÃ¡pida
"""
verify_mlflow_connection()
"""

# 3. Pipeline completo
"""
run_complete_mlflow_pipeline()
"""

# 4. Solo entrenamiento
"""
dataset_path = find_dataset()
df = load_and_explore_dataset(dataset_path)
X, y, features = prepare_features_for_training(df)
model, run_id, accuracy = train_model_with_mlflow(X, y, features)
"""

# 5. Solo predicciones
"""
results, pred_run_id = make_predictions_with_mlflow_model(run_id="TU_RUN_ID")
"""

# 6. Comparar experimentos
"""
comparison_df = compare_experiments()
feature_analysis = analyze_feature_importance()
"""

# 7. DiagnÃ³stico
"""
troubleshoot_mlflow()
"""
```

---

## ğŸ¯ RESULTADO ESPERADO

Al final de esta guÃ­a tendrÃ¡s:

âœ… **MLflow funcionando** con tracking completo
âœ… **Modelo entrenado** con mÃ©tricas registradas  
âœ… **Predicciones generadas** con evaluaciÃ³n
âœ… **Experimentos comparables** en UI web
âœ… **Artefactos guardados** (modelo, reportes, datos)
âœ… **Pipeline reproducible** para futuras iteraciones

**ğŸŒ Accede a MLflow UI**: http://localhost:5000

**ğŸ“Š Revisa tus experimentos, mÃ©tricas, y modelos registrados!**

---

*GuÃ­a creada: Julio 2025 | Chess Trainer ML Pipeline*
