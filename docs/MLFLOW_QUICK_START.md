# ğŸš€ MLflow Quick Start - Chess Trainer

## ğŸ“‹ Â¿QuÃ© acabamos de configurar?

Hemos configurado **MLflow** completamente integrado con tu proyecto chess_trainer. MLflow es una herramienta que te permite:

- ğŸ”¬ **Trackear experimentos**: Cada vez que entrenes un modelo, MLflow registra automÃ¡ticamente parÃ¡metros, mÃ©tricas y resultados
- ğŸ“Š **Comparar modelos**: Ver fÃ¡cilmente quÃ© modelo funciona mejor para predecir errores en ajedrez
- ğŸ¯ **Reproducir resultados**: Volver a ejecutar experimentos exactos con los mismos parÃ¡metros
- ğŸš€ **Desplegar modelos**: Servir tu mejor modelo para hacer predicciones en producciÃ³n

## ğŸ³ Iniciar MLflow (Primera vez)

### OpciÃ³n 1: Usar VS Code Tasks (Recomendado)
```
Ctrl+Shift+P â†’ "Tasks: Run Task" â†’ "ğŸš€ ML Workflow: Start MLflow Server"
```

### OpciÃ³n 2: PowerShell Script
```powershell
.\build_up_clean_all.ps1
```

### OpciÃ³n 3: Docker Compose Manual
```bash
docker-compose up -d mlflow
```

## ğŸŒ Acceder a MLflow UI

Una vez iniciado, puedes ver la interfaz web en:
- **URL**: http://localhost:5000
- **VS Code Task**: `ğŸŒ ML Workflow: Open MLflow UI`

## ğŸ§ª Ejecutar tu Primer Experimento

### 1. ConfiguraciÃ³n Inicial
```bash
# Desde VS Code Terminal:
docker-compose exec notebooks python /chess_trainer/src/scripts/setup_mlflow.py
```

### 2. Entrenar Modelos de Ejemplo
```bash
# Entrenar modelos baseline (RandomForest, LogisticRegression, SVM):
docker-compose exec notebooks python /chess_trainer/src/ml/train_error_model.py
```

## ğŸ“Š QuÃ© VerÃ¡s en MLflow UI

### **Experimentos Creados**:
1. **chess_error_classification** - Predecir tipos de error (blunder, mistake, inaccuracy)
2. **chess_feature_engineering** - Probar diferentes combinaciones de features
3. **chess_hyperparameter_tuning** - Optimizar parÃ¡metros de modelos
4. **chess_model_comparison** - Comparar diferentes algoritmos
5. **chess_phase_analysis** - Modelos especÃ­ficos por fase del juego
6. **chess_elo_analysis** - AnÃ¡lisis por rango de ELO

### **MÃ©tricas Tracked**:
- `accuracy` - PrecisiÃ³n general del modelo
- `f1_macro` - F1-score promedio entre todas las clases
- `precision_macro` - PrecisiÃ³n promedio
- `recall_macro` - Recall promedio
- `cv_accuracy_mean` - PrecisiÃ³n de validaciÃ³n cruzada
- `feature_importance_*` - Importancia de cada feature

### **Artefactos Guardados**:
- ğŸ“ˆ **GrÃ¡ficos**: Matriz de confusiÃ³n, importancia de features
- ğŸ“„ **Reportes**: Classification report, resultados detallados
- ğŸ¤– **Modelos**: Modelos entrenados listos para usar
- ğŸ“Š **Datasets**: InformaciÃ³n sobre datos utilizados

## ğŸ¯ Casos de Uso Inmediatos

### **Comparar Algoritmos**
```python
# En MLflow UI, ve a "chess_error_classification"
# Compara mÃ©tricas entre RandomForest, LogisticRegression y SVM
# Ordena por "accuracy" para ver el mejor modelo
```

### **Optimizar HiperparÃ¡metros**
```python
# Ve a "chess_hyperparameter_tuning"
# Observa cÃ³mo diferentes parÃ¡metros afectan el rendimiento
# Encuentra la mejor combinaciÃ³n de n_estimators, max_depth, etc.
```

### **Analizar Features**
```python
# En cualquier experimento con RandomForest
# Ve "Artifacts" â†’ "feature_importance_plot.png"
# Descubre quÃ© features son mÃ¡s importantes para predecir errores
```

## ğŸ”„ Workflow TÃ­pico

### 1. **Experimentar**
```bash
# Modificar parÃ¡metros en train_error_model.py
# Ejecutar entrenamiento
docker-compose exec notebooks python /chess_trainer/src/ml/train_error_model.py
```

### 2. **Comparar en UI**
- Abrir http://localhost:5000
- Seleccionar experimento
- Comparar runs por mÃ©tricas
- Revisar grÃ¡ficos y reportes

### 3. **Iterar**
- Identificar quÃ© funciona mejor
- Probar nuevas combinaciones
- Optimizar hiperparÃ¡metros
- Agregar nuevas features

### 4. **Desplegar**
```python
# Cargar mejor modelo para uso en producciÃ³n
import mlflow.sklearn
model = mlflow.sklearn.load_model("runs:/<run_id>/model")
predictions = model.predict(new_data)
```

## ğŸ“ˆ PrÃ³ximos Pasos Sugeridos

### **Inmediato (Hoy)**:
1. âœ… Ejecutar `setup_mlflow.py` - Crear experimentos base
2. âœ… Ejecutar `train_error_model.py` - Entrenar modelos baseline
3. âœ… Explorar MLflow UI - Familiarizarte con la interfaz
4. âœ… Comparar modelos - Ver cuÃ¡l funciona mejor

### **Esta Semana**:
1. ğŸ¯ Probar con tus datasets reales de chess
2. ğŸ”§ Optimizar hiperparÃ¡metros del mejor modelo
3. ğŸ“Š Analizar importancia de features tÃ¡cticos
4. ğŸš€ Integrar modelo en tu FastAPI

### **PrÃ³ximo Sprint**:
1. ğŸ§  Modelos especializados por fase de juego
2. ğŸ“ˆ Tracking de mejora de jugadores en el tiempo
3. ğŸ® IntegraciÃ³n con sistema de recomendaciones
4. ğŸ”„ Pipeline automatizado de reentrenamiento

## ğŸ› SoluciÃ³n de Problemas

### **MLflow UI no carga**:
```bash
# Verificar que el servicio estÃ© corriendo
docker-compose ps mlflow

# Ver logs del servicio
docker-compose logs -f mlflow

# Reiniciar si es necesario
docker-compose restart mlflow
```

### **Error al ejecutar scripts**:
```bash
# Verificar que el contenedor de notebooks estÃ© activo
docker-compose ps notebooks

# Acceder al contenedor para debug
docker-compose exec notebooks bash
cd /chess_trainer
python src/scripts/setup_mlflow.py
```

### **No hay datasets**:
```bash
# Generar features si no existen
docker-compose exec chess_trainer python src/scripts/generate_features_parallel.py

# Exportar a formato parquet
docker-compose exec chess_trainer python src/scripts/export_features_dataset_parallel.py
```

## ğŸ‰ Â¡Listo para Experimentar!

Ya tienes todo configurado para empezar a usar Machine Learning de forma profesional en tu proyecto de ajedrez. MLflow te ayudarÃ¡ a:

- ğŸ¯ **Enfocar** tus experimentos y no perder resultados
- ğŸ“Š **Comparar** objetivamente diferentes enfoques  
- ğŸš€ **Accelerar** el desarrollo de nuevos modelos
- ğŸ”„ **Reproducir** resultados cuando necesites volver a un modelo anterior

**Â¡Experimenta, compara y mejora tus modelos de ajedrez!** ğŸ†

---

*Creado: 2025-01-26*  
*Proyecto: chess_trainer + MLflow + Docker*
