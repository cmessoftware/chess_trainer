# Shared Volumes Configuration for Datasets

## Description

A shared volume (`chess_datasets`) has been configured to share Parquet datasets generated by the main application container with the notebooks container.

## Quick Setup

### Windows Users (Recommended):
```powershell
.\build_up_clean_all.ps1
```
This script automatically builds both containers with proper volume configuration and starts all services.

#### ðŸŽ¯ Benefits for Dataset Management:
- **Automatic Volume Binding**: Ensures correct shared volume (`chess_datasets`) configuration
- **Container Synchronization**: Both app and notebooks containers access the same dataset location
- **No Manual Volume Setup**: Eliminates need for manual Docker volume creation and binding
- **Data Persistence**: Datasets remain available across container restarts and rebuilds
- **Cross-Container Access**: Seamless data sharing between main application and Jupyter notebooks
- **Cleanup Integration**: Removes unused volumes during cleanup process

### Manual Setup:
```bash
docker-compose build
docker-compose up -d
```

## Volume Structure

```
docker-compose.yml
â”œâ”€â”€ chess_trainer (app container)
â”‚   â””â”€â”€ /app/src/data -> chess_datasets volume
â””â”€â”€ notebooks container  
    â””â”€â”€ /notebooks/datasets -> chess_datasets volume
```

## Dataset Locations

### In the application container (`chess_trainer`):
- **Path**: `/app/src/data/`
- **Important subdirectories**:
  - `/app/src/data/processed/` - Final combined datasets
  - `/app/src/data/export/` - Datasets by source (personal, novice, elite, etc.)

### In the notebooks container:
- **Path**: `/notebooks/datasets/`
- **Access from notebooks**:
  ```python
  import pandas as pd
  
  # Read combined dataset
  df = pd.read_parquet("/notebooks/datasets/processed/training_dataset.parquet")
  
  # Read datasets by source
  df_elite = pd.read_parquet("/notebooks/datasets/export/elite/features.parquet")
  df_personal = pd.read_parquet("/notebooks/datasets/export/personal/features.parquet")
  ```

## Scripts that Generate Datasets

### 1. `export_features_dataset_parallel.py`
- **Function**: Exports features filtered by source, player, opening, ELO
- **Output**: `/app/src/data/export/{source}/features.parquet`
- **Usage**:
  ```bash
  python src/scripts/export_features_dataset_parallel.py --source elite --limit 10000
  ```

### 2. `generate_combined_dataset.py`
- **Function**: Combines datasets from multiple sources into one
- **Input**: `/app/src/data/processed/{source}_games.parquet`
- **Output**: `/app/src/data/processed/training_dataset.parquet`
- **Usage**:
  ```bash
  python src/scripts/generate_combined_dataset.py
  ```

### 3. `generate_features_parallel.py`
- **Function**: Generates features from games stored in PostgreSQL
- **Stores**: Features in the database, exportable with previous scripts

## Docker Commands

### Start containers:
```bash
docker-compose up -d
```

### View shared volume:
```bash
docker volume inspect chess_trainer_chess_datasets
```

### Access app container to generate datasets:
```bash
docker exec -it chess_trainer_chess_trainer_1 bash
cd /app
python src/scripts/export_features_dataset_parallel.py --source elite
```

### Access notebooks container:
```bash
docker exec -it chess_trainer_notebooks_1 bash
ls -la /notebooks/datasets/
```

## Example Usage in Notebook

```python
import pandas as pd
import os

# Check available files
datasets_path = "/notebooks/datasets"
print("Available datasets:")
for root, dirs, files in os.walk(datasets_path):
    for file in files:
        if file.endswith('.parquet'):
            print(f"  {os.path.join(root, file)}")

# Load main dataset
df_training = pd.read_parquet(f"{datasets_path}/processed/training_dataset.parquet")
print(f"Training dataset: {len(df_training)} rows")

# Load specific dataset by source
df_elite = pd.read_parquet(f"{datasets_path}/export/elite/features.parquet")
print(f"Elite dataset: {len(df_elite)} rows")
```

## Recommended Workflow

1. **Feature Generation** (in app container):
   ```bash
   python src/scripts/generate_features_parallel.py --source elite --max-games 1000
   ```

2. **Export to Parquet** (in app container):
   ```bash
   python src/scripts/export_features_dataset_parallel.py --source elite
   ```

3. **Combined Dataset Generation** (in app container):
   ```bash
   python src/scripts/generate_combined_dataset.py
   ```

4. **Analysis in Notebooks** (in notebooks container):
   - Open Jupyter at http://localhost:8888
   - Create new notebook
   - Load data from `/notebooks/datasets/`

## Advantages of this Configuration

- âœ… **Persistence**: Datasets persist between container restarts
- âœ… **Sharing**: Both containers access the same data
- âœ… **Synchronization**: Datasets generated in app are immediately available in notebooks
- âœ… **Separation**: Datasets don't mix with source code
- âœ… **Scalability**: Easy to add more containers that need dataset access