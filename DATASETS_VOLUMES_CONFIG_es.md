# Configuraci√≥n de Vol√∫menes Compartidos para Datasets

## Descripci√≥n

Se ha configurado un volumen compartido (`chess_datasets`) para compartir los datasets Parquet generados por el contenedor de la aplicaci√≥n principal con el contenedor de notebooks.

## Configuraci√≥n R√°pida

### Usuarios de Windows (Recomendado):
```powershell
.\build_up_clean_all.ps1
```
Este script construye autom√°ticamente ambos contenedores con la configuraci√≥n de vol√∫menes adecuada e inicia todos los servicios.

#### üéØ Beneficios para la Gesti√≥n de Datasets:
- **Vinculaci√≥n Autom√°tica de Vol√∫menes**: Asegura la configuraci√≥n correcta del volumen compartido (`chess_datasets`)
- **Sincronizaci√≥n de Contenedores**: Tanto la aplicaci√≥n como los notebooks acceden a la misma ubicaci√≥n de datasets
- **Sin Configuraci√≥n Manual de Vol√∫menes**: Elimina la necesidad de creaci√≥n y vinculaci√≥n manual de vol√∫menes Docker
- **Persistencia de Datos**: Los datasets permanecen disponibles entre reinicios y reconstrucciones de contenedores
- **Acceso Entre Contenedores**: Intercambio de datos sin problemas entre la aplicaci√≥n principal y los notebooks Jupyter
- **Limpieza Integrada**: Elimina vol√∫menes no utilizados durante el proceso de limpieza

### Configuraci√≥n Manual:
```bash
docker-compose build
docker-compose up -d
```

## Estructura de Vol√∫menes

```
docker-compose.yml
‚îú‚îÄ‚îÄ chess_trainer (app container)
‚îÇ   ‚îî‚îÄ‚îÄ /app/src/data -> chess_datasets volume
‚îî‚îÄ‚îÄ notebooks container  
    ‚îî‚îÄ‚îÄ /notebooks/datasets -> chess_datasets volume
```

## Ubicaci√≥n de los Datasets

### En el contenedor de la aplicaci√≥n (`chess_trainer`):
- **Ruta**: `/app/src/data/`
- **Subdirectorios importantes**:
  - `/app/src/data/processed/` - Datasets combinados finales
  - `/app/src/data/export/` - Datasets por fuente (personal, novice, elite, etc.)

### En el contenedor de notebooks:
- **Ruta**: `/notebooks/datasets/`
- **Acceso desde notebooks**:
  ```python
  import pandas as pd
  
  # Leer dataset combinado
  df = pd.read_parquet("/notebooks/datasets/processed/training_dataset.parquet")
  
  # Leer datasets por fuente
  df_elite = pd.read_parquet("/notebooks/datasets/export/elite/features.parquet")
  df_personal = pd.read_parquet("/notebooks/datasets/export/personal/features.parquet")
  ```

## Scripts que Generan Datasets

### 1. `export_features_dataset_parallel.py`
- **Funci√≥n**: Exporta features filtrados por fuente, jugador, apertura, ELO
- **Salida**: `/app/src/data/export/{source}/features.parquet`
- **Uso**:
  ```bash
  python src/scripts/export_features_dataset_parallel.py --source elite --limit 10000
  ```

### 2. `generate_combined_dataset.py`
- **Funci√≥n**: Combina datasets de m√∫ltiples fuentes en uno solo
- **Entrada**: `/app/src/data/processed/{source}_games.parquet`
- **Salida**: `/app/src/data/processed/training_dataset.parquet`
- **Uso**:
  ```bash
  python src/scripts/generate_combined_dataset.py
  ```

### 3. `generate_features_parallel.py`
- **Funci√≥n**: Genera features desde los juegos almacenados en PostgreSQL
- **Almacena**: Features en la base de datos, exportables con scripts anteriores

## Comandos Docker

### Iniciar los contenedores:
```bash
docker-compose up -d
```

### Ver el volumen compartido:
```bash
docker volume inspect chess_trainer_chess_datasets
```

### Acceder al contenedor de la app para generar datasets:
```bash
docker exec -it chess_trainer_chess_trainer_1 bash
cd /app
python src/scripts/export_features_dataset_parallel.py --source elite
```

### Acceder al contenedor de notebooks:
```bash
docker exec -it chess_trainer_notebooks_1 bash
ls -la /notebooks/datasets/
```

## Ejemplo de Uso en Notebook

```python
import pandas as pd
import os

# Verificar archivos disponibles
datasets_path = "/notebooks/datasets"
print("Datasets disponibles:")
for root, dirs, files in os.walk(datasets_path):
    for file in files:
        if file.endswith('.parquet'):
            print(f"  {os.path.join(root, file)}")

# Cargar dataset principal
df_training = pd.read_parquet(f"{datasets_path}/processed/training_dataset.parquet")
print(f"Dataset de entrenamiento: {len(df_training)} filas")

# Cargar dataset espec√≠fico por fuente
df_elite = pd.read_parquet(f"{datasets_path}/export/elite/features.parquet")
print(f"Dataset elite: {len(df_elite)} filas")
```

## Flujo de Trabajo Recomendado

1. **Generaci√≥n de Features** (en app container):
   ```bash
   python src/scripts/generate_features_parallel.py --source elite --max-games 1000
   ```

2. **Exportaci√≥n a Parquet** (en app container):
   ```bash
   python src/scripts/export_features_dataset_parallel.py --source elite
   ```

3. **Generaci√≥n de Dataset Combinado** (en app container):
   ```bash
   python src/scripts/generate_combined_dataset.py
   ```

4. **An√°lisis en Notebooks** (en notebooks container):
   - Abrir Jupyter en http://localhost:8888
   - Crear nuevo notebook
   - Cargar datos desde `/notebooks/datasets/`

## Ventajas de esta Configuraci√≥n

- ‚úÖ **Persistencia**: Los datasets persisten entre reinicios de contenedores
- ‚úÖ **Compartici√≥n**: Ambos contenedores acceden a los mismos datos
- ‚úÖ **Sincronizaci√≥n**: Los datasets generados en app est√°n inmediatamente disponibles en notebooks
- ‚úÖ **Separaci√≥n**: Los datasets no se mezclan con el c√≥digo fuente
- ‚úÖ **Escalabilidad**: F√°cil agregar m√°s contenedores que necesiten acceso a los datasets
